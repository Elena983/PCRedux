---
title: "PCRedux Package - An Overview"
author: "Stefan R&ouml;diger"
date: "`r Sys.Date()`"
output: 
  rmarkdown::pdf_document:
    number_sections: true
    toc: true
bibliography: "literature.bib"
vignette: >
  %\VignetteIndexEntry{PCRedux package - an overview}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

\begin{figure}[ht]
\centering
\scalebox{0.6}{
\includegraphics[clip=true,trim=1cm 1cm 1cm 1cm]{Logo.pdf}
}
\end{figure}

## Development, Implementation and Installation \label{section_Development_Implementation_and_Installation}

\texttt{PCRedux} is an open source software package for the statistical computing language \texttt{R}. This software is published under the terms of the [MIT license](https://opensource.org/licenses/MIT)\footnote{\url{https://opensource.org/licenses/MIT}}. \texttt{PCRedux} contains function for the calculation of features from amplification curves and classified data sets for machine learning applications.


Reproducibility is a foundation of research. All technical and experimental aspects should be performed under principles that follow good practices of reproducible research. Numerous authors addressed the matter for experimental design and data report. Examples are the *Minimum Information for Publication of Quantitative PCR Experiments* guidelines (MIQE) and the *Real-time PCR Data Markup Language* (RDML). MIQE is a recommended standard of the minimum information for publication of quantitative real-time PCR experiments guidelines and RDML is a data exchange format [@bustin_reproducibility_2014, @bustin_continuing_2017, @roediger2015r, @roediger_enabling_2017, @wilson_good_2016]. Both MIQE and RDML, are widely used to preform quantitative real-time PCRs [@pabinger_2014].

The development of scientific software is a complex process. In particular, if the development is carried out by teams who work in different time zones and where no face-to-face meetings a possible. End users need releases with stable software that delivers reproducible results. Developers need well documented software the adopt the software according to their needs. Under the umbrella \textit{Agile Software Development} and \textit{Extreme Programming}, several principles were proposed to deliver high quality software, which meet the needs of end users and developers. This includes version control, collaborative editing, unit testing and continuous integration[@lanubile_collaboration_2010, @myers_art_2004, @roediger2015r]. The following paragraphs describe methods implemented in the PCRedux package to ensure high software quality.

### Version Control and Continuous Integration \label{section_Version_Control_and_Continuous_Integration}

The development of the \texttt{PCRedux} package started 2017 with the submission of a functional, yet immature source code, to GitHub (GitHub, Inc.). GitHub is a web-based version control repository hosting service. Both distributed version control and source code management are based on Git. [@lanubile_collaboration_2010].  Additional functionality of GitHub includes the administration of access management, bug tracking, moderation of feature requests, task management, some metrics for the software development, and wikis. The source code of \texttt{PCRedux} is available at:

> \url{https://github.com/devSJR/PCRedux/}

In continuous integration development team members commit and integrate their contributions several times a day. Team members may include coders, artists and translators. An automated build and test system verifies each integration and gives the development team members a timely feedback about the effect of their commit. In contrast to deferred integration leads this to a reduced number of integration problems and less workload because most erros are solved shortly after they were integrated [@myers_art_2004].

TrivsCI was chosen as continues integration service for \texttt{PCRedux}. The TravisCI server communicates with the GitHub version control system and manages the  \texttt{PCRedux} package building process. Currently the continuous interaction is available for the \texttt{R} releases \textit{oldrel}, \textit{release} and \textit{devel}. The history of the build tests are available at

> \url{https://travis-ci.org/devSJR/PCRedux}

### Naming Convention and Literate Programming \label{section_Naming_Convention_and_Literate_Programming}

The \texttt{PCRedux} software is provided as an \text{R} ($\geq$ v. 3.3.3) package.  \texttt{PCRedux} is written as \textit{S3} object system. \textit{S3} has characteristics of object orientated programming but eases the development due to the use of the naming conventions [@noauthor_compstat_2008]. In most places function and parameter names are written as underscore separated (underscore$\_$sep), which is a widely used style in \texttt{R} packages [@Baaaath_2012]. This convention had to be violated in coding sections where functionality from other packages was used.

Literate programming, as proposed by @knuth_literate_1984, is a concept where the logic of the source code and documentation is integrated in a single file. Markup conventions (e.g., '#') tell in literate programming how to typeset the documentation. This produces outputs in a typesetting language such as the lightweight markup language **Markdown**, or the document preparation system  \LaTeX. 

The \texttt{roxygen2}, \texttt{rmarkdown} and \texttt{knitr} packages were used to write the documentation in-line with code for the \texttt{PCRedux} package.

### Installation of the \texttt{PCRedux} Package \label{section_Installation_of_the_PCRedux_Package}

The development version of the package can be installed using the The developer version of the package can be installed using the \texttt{devtools} package.

```{r, eval=FALSE, echo=TRUE}
# Install devtools, if you haven't already.
install.packages("devtools")

library(devtools)
install_github("devSJR/PCRedux")
```

\texttt{PCRedux} is available as stable version from the \textbf{C}omprehensive \texttt{R} \textbf{A}rchive \textbf{N}etwork (CRAN) at \url{https://CRAN.R-project.org/package=PCRedux}. Package published at CRAN undergoe intensive checking procedures. In addition, CRAN tests whether the package can be built for common operating systems and whether all version dependencies are solved. To install \texttt{PCRedux} first install \texttt{R} ($\geq$ v. 3.3.3). Then start \texttt{R} and type in the prompt:

```{r, eval=FALSE, echo=TRUE}
# Select your local mirror
install.packages("PCRedux")
```

The \texttt{PCRedux} package should just install. If this fails make sure you that write access is permitted to the destination directory.

```{r, eval=FALSE, echo=TRUE}
# The following command points to the help for download and install of packages
# from CRAN-like repositories or from local files.
?install.packages()
```

If this fails try to follow the instructions given  by @de_vries_r_2012.

> R CMD check 

Results from CRAN check can be found at 

> \url{http://cran.us.r-project.org/web/checks/check_results_PCRedux.html}.


### Unit Testing of the \texttt{PCRedux} Package \label{section_Unit_Testing_of_the_PCRedux_Package}

Modules testing, better known as unit testing, is an approach to simplify the refactoring of source code during software development. The goal is to minimize errors and regressions. It is also intended to ensure that the numerical results from the calculations are reproducible and of high quality. An unintended behavior of the software should be detected at the latest during the package building process. Please note that Unit Testing is not a guarantee for error-free software [@myers_art_2004].

The basic concept is to use checkpoints to check whether the software performs calculations and data transformations correctly for all builds. For this, numerous (logical) queries have to be defined by the developer in advance. They are refereed to \textit{expectations}. It should be ensured that as many errors as possible are covered. A logical query can be, for example, whether the calculation has a numeric or Boolean value as output. If the data type is incorrect during output, this is a sufficient termination criterion. Or it can be checked whether the length of the result vector is correct after the calculation. There are different approaches for unit tests in \texttt{R}. This also includes testing of units from the packages \texttt{RUnit}, \texttt{covr}, \texttt{svUnit} and \texttt{testthat}. (@wickham_testthat_2011).

The package \texttt{testthat} was used in \texttt{PCRedux} because it could be well implemented and its maintenance is relatively simple. The logic is that an \textit{expectation} defines how the result, class or error in the corresponding unit (e.g., function) should behave. Unit tests can be found in the `/test/testthat` subdirectory of the \texttt{PCRedux} package. The unit tests always run automatically during the creation of the package. The following is an example of the function ``qPCR2fdata()``. The details of how ``qPCR2fdata()`` works are detailed in the \autoref{section_qPCR2fdata} section. The function ``test_that()``, from the \texttt{testthat} package, is given several \textit{expectations}. The ``qPCR2fdata()`` function when processing the amplification curves check whether:

* an object of the class *fdata* is created (see @Febrero_Bande_2012 for details of the class *fdata*),
* the parameter *rangeval* has a length of two,
* is the second value of parameter *Rangeval* 49 (last cycle number) and 
*whether the object structure of the function ``qPCR2fdata()`` does not change if the parameter \textit{preprocess=TRUE} is set.

```{r, eval=FALSE, echo=TRUE}
library(PCRedux)

context("qPCR2fdata")

test_that("qPCR2fdata gives the correct dimensions and properties", {
  library(qpcR)
  res_fdata <- qPCR2fdata(testdat)
  res_fdata_preprocess <- qPCR2fdata(testdat, preprocess = TRUE)

  expect_that(res_fdata, is_a("fdata"))
  expect_that(length(res_fdata$rangeval) == 2 &&
    res_fdata$rangeval[2] == 49, is_true())

  expect_that(res_fdata_preprocess, is_a("fdata"))
  expect_that(length(res_fdata_preprocess$rangeval) == 2 &&
    res_fdata_preprocess$rangeval[2] == 49, is_true())
})
```

Similar unit tests were implemented for all functions of the \texttt{PCRedux} package. The coverage by \texttt{PCRedux} package can be calculated by the ``package_coverage()`` function from the \texttt{covr} package or visual analyzed at 

> \url{https://codecov.io/gh/devSJR/PCRedux/list/master/}.

## Data Sets of Amplification Curves and Classification\label{s}

An extensive literature research showed that in the field of qPCR there are no openly accessible data sets. Open Data is meant in the sense that data are freely available, free of charge, free to use and that data can be republished, without restrictions from copyright, patents or other mechanisms of control [@kitchin2014]. Furthermore, only a few attributes of amplification curves are described in the literature (see \autoref{amplification_curve_forms}). That includes them:

* the signal height and the slope in the baseline region (gradient and intersection),
* the starting point of amplification,
* the Cq value and amplification efficiency, and 
* the signal level including the slope of the plateau phase (Slope, Intercept).

However, these alone are presumably not enough to describe amplification curves sufficiently. Furthermore, there are no references to further algorithms that can be used to calculate additional features from amplification curves. All these facts make further studies on machine learning and modeling difficult. A feature can be described as an entity that characterizes an object. The number of features should be large enough to describe the object accurately and small enough not to interfere with the learning process with redundant or information. 

Bellman coined the so-called *Curse of Dimensionality* in 1961, when he dealt with adaptive control processes. It vague describes the practical difficulties encountered in high-dimensional analysis and estimation.  It states that for a given sample size, there is a maximum number of features from which the performance of an algorithm degrades rather than improves. 
As a consequence, many data mining algorithms fail when the dimensionality is high, because the data points are sparsely populated and far apart [@herrera_multiple_2016].

Therefore, a large number of records with amplification curves and their classification (negative, ambiguous, positive) were included in the \texttt{PCRedux} package. Another objective was the development of new algorithms and the transfer of algorithms from other domains (e.g., from digital image processing) to qPCR data sets.  A central goal was therefore to develop attributes to enable the classification of amplification curves in categories such as positive, negative and ambiguous.

It is worth noting that the classifications of amplification curves in \autoref{table-datasets} were made on the basis of empirical values. For the amplification curves, only an assessment was made to see if the curves are approximately sigmoid or resemble a negative amplification reaction with a flat curve shape. Consequently, this does not answer the question of if a specific amplification product has been synthesized, if a contamination has been amplified or if only primer-dimers have been amplified. To answer this question, other methods such as melting curve analysis should be used.

Amplification curves from different sources had to be classified manually. Amplification curves from the \texttt{qpcR}, \texttt{chipPCR}, \texttt{PCRedux} and \texttt{RDML} packages were classified with the ``humanrater()``, as described in @roediger2015chippcr and with the ``tReem()`` function from the \texttt{PCRedux} package. This step is briefly outlined in the chapter \autoref{humanrater}.

Data preparation is an important step. This includes the steps involved in data cleansing, data transformation and data integration[@herrera_multiple_2016]. The \texttt{xray} package [@Seibelt_xray] can used to analyze the distribution form and variables in records for anomalies such as missing values, zeros, infinite values and their categories. The ``anomalies()`` function from the \texttt{xray} package can be used to search for anomalies (including missing values (NA), zero values (Zero), blank strings (Blank) and infinite numbers (Inf)). 

**Note**: All users of the \texttt{PCRedux} package should use such tools before continuing to work with the records. Although most records in the \texttt{PCRedux} package have the same data structure, some records contain missing values or have different dimensions (compare data from \autoref{samples_of_qPCRs}). For example, the data set `C127EGHP` spans a matrix of `r data_dim <- dim (chipPCR::C127EGHP); paste0(data_dim[1]," x ", data_dim[2])`, while the `htPCR` data set comprises a matrix of `r data_dim <- dim (qpcR:: htPCR); paste0 (data_dim[1]," x ", data_dim[2])`.

Raw data were exported as comma separated values from the thermal cyclers. Some records have been exported from the devices using the \texttt{RDML} package and transformed into RDML format. A detailed description can be found in @roediger_enabling_2017. The Real-time PCR Data Markup Language (RDML) is data exchange format for quantitative Real-Time PCR Experiments. RDML is a human readable file format and is based on XML (eXtensible Markup Language) and was created to enable the exchange of data across different information systems [@lefever_rdml_2009]. The following code section describes the import of an RDML file from the \texttt{PCRedux} package. The RDML file contains amplification curve data of a duplex qPCR (HPV 16 & HPV 18) performed in the CFX96 (Bio-Rad).

```{r, echo=TRUE, eval=FALSE}
library(RDML)
# Load the RDML package and use its functions to import the amplification curve
#  data
library(RDML)
filename <- system.file("RAS002.rdml", package = "PCRedux")
raw_data <- RDML$new(filename = filename)
```
The further processing of the amplification data took place as described in @roediger2015chippcr, @roediger2015r, @spiess_impact_2015 and @spiess_system-specific_2016. An introduction to the use of \texttt{R} for the analysis of melting curves (\texttt{MBmca} package, [@roediger_RJ_2013]) and the calculation of Cq values (\texttt{chipPCR} package, [@roediger2015chippcr]) is shown in detail in @roediger2015r. Unless otherwise stated, the Cq values were determined using the second maximum derivative method.

### The ``tReem()`` Function and the Classified Amplification Curve Data Sets \label{chapter_humanrater}

For machine learning and method validation it was important to classify the amplification curves individually. In @roediger2015chippcr the  ``humanrater()``  function was introduced. This tool to assist the human expert during the classification of amplification curves and melting curves. The human expert has to define classes (e.g., negative ("n"), ambiguous ("a"), positive ("p")) which get assigned to an amplification curve after expert has entered the class in input mask. All amplification curve data sets listed in \autoref{table-datasets} were classified in interactive, semi-blinded sessions.  ``humanrater()`` was set to randomly select individual amplification curves. All classifications were done in at least three repeats. The classification of the `htPCR` data set was done in total eight times  (see \autoref{figure_curve_classification}) because most of the amplification curves are neither unequivocal classifiable as positive or negative (see \autoref{samples_of_qPCRs}B).

```{r, echo=TRUE, eval=TRUE, fig.cap="\\label{figure_curve_classification} Class frequency. Amplification curves from the `htPCR` data set (\\texttt{qpcR} package, [@Ritz2008]) were classified by a human eight times with the classes ambiguous (a), positive (y) or negative (n)."}
# Suppress messages and load the packages for reading the data of the classified
# amplification curves.
options(warn = -1)
library(PCRedux)
suppressMessages(library(data.table))

# Load the decision_res_htPCR.csv data set from a csv file.
filename <- system.file("decision_res_htPCR.csv", package = "PCRedux")
decision_htPCR <- fread(filename, data.table = FALSE)

# 
par(mfrow=c(2,4))
for(i in 2L:9) {
data_tmp <- table(as.factor(decision_htPCR[, i]))

barplot(data_tmp)
text(c(0.7, 1.9, 3.1), rep(quantile(data_tmp, 0.25), 3), data_tmp, srt=90)
mtext(LETTERS[i - 1], cex = 1.5, side = 3, adj = 0, font = 2)
}
```

The ``humanrater()`` function[@roediger2015chippcr] was developed with the aim that amplification curves are taken individually (randomly) from the data sets and presented by a human expert for classification. On the basis of his or her knowledge, the human expert is then able to undertake a classification. Basically, this approach is well suited and has been applied to a variety of amplification curves during the development of the \texttt{PCRedux} package. This methodological approach can, however, be very time-consuming, depending on the size of the data set. In addition, this approach can also be tiring for large datasets, especially when the amplification curves are very similar. A high similarity between amplification curves exists, for example, in replicates and negative controls.

In theory, the similarity between the amplification curves can also be used to form groups with very similar curves. The amplification curves in the groups can then be classified together in one run. In this way, a higher throughput can be achieved for classification. This approach has not yet been described for the analysis of qPCR data in the literature.

The ``tReem()`` function was developed to perform a curve-shape based classification. Two algorithms have been integrated in the ``tReem()`` function to quantify the similarity between amplification curves. The interface of the ``tReem()`` function is similar to that of the ``humanrater()`` function. The function ``tReem()`` needs a data structure where the first column contains the qPCR cycles and all other columns contain the amplification curves. After a chain of processing steps, the ``tReem()`` function presents the human expert with a series of plots with a single amplification curve (no similarity to other curves) or groups of amplification curves (within a group there is a high similarity). The corresponding classes can then be assigned to the groups of amplification curves by the human expert using an input mask.

In the first method (standard), the correlation coefficients (*r*) are determined in pairs according to Pearson for all combinations of amplification curves. The correlation calculation is used to describe the strength of the correlation between the two variables in a statistical measure. Consequently, the correlation coefficient *r* can be regarded as a distance between the amplification curves. *r* is a dimensionless value and only takes values between -1 and 1. If *r = -1*, there is a maximum reciprocal relationship. If *r = 0* there is no correlation between the two variables. If *r = 1*, there is a maximum rectified correlation. 

In the second method, the Hausdorff distance is used to determine the similarity between amplification curves. The Hausdorff distance is the "the maximum of the distances from a point in any of the sets to the nearest point in the other set" [@rote_computing_1991, @herrera_multiple_2016]. The amplification curves are converted within the ``tReem()`` function using the ``qPCR2data()`` function.

Both methods process the distances in the same steps. This involves the calculation of the distance matrix using the Euclidean distances of all distance measures to determine the distance between the lines of the data matrix. This is used to perform a hierarchical cluster analysis. In the last step, the cluster is divided into groups based on a user-defined *k* value. For example, two groups are created for *k = 2*. If the amplification curves are very different, a larger *k* should be used.

As a rule, the grouping of the amplification curves using the Pearson correlation coefficient as a distance measure is faster than the Hausdorff distance. Nevertheless, it is up to the user to find the optimal method for his task.

Ideally, only a few iterations are necessary to complete the classification of a data set. However, a prerequisite for this is that the amplification curves are similar.


```{r, echo=TRUE, eval=FALSE}
# Classify amplification curve data by correlation coefficients (r)
library(qpcR)
tReem(testdat[, 1:15], k = 3)
```

\begin{table}[]
\centering
\caption{Classified amplification curve data sets.}
\label{table-datasets}
\begin{tabular}{lll}
\hline
Decision Data Sets in PCRedux                    & qPCR Data Set               & Package       \\ \hline
decision\_res\_RAS002.csv                        & RAS002.rdml                 & \texttt{PCRedux} \\
decision\_res\_RAS003.csv                        & RAS003.rdml                 & \texttt{PCRedux} \\
decision\_res\_batsch1.csv                       & batsch1                     & \texttt{qpcR}    \\
decision\_res\_batsch2.csv                       & batsch2                     & \texttt{qpcR}    \\
decision\_res\_batsch3.csv                       & batsch3                     & \texttt{qpcR}    \\
decision\_res\_batsch4.csv                       & batsch4                     & \texttt{qpcR}    \\
decision\_res\_batsch5.csv                       & batsch5                     & \texttt{qpcR}    \\
decision\_res\_lc96\_bACTXY.csv                  & lc96\_bACTXY.rdml           & \texttt{RDML}    \\
decision\_res\_boggy.csv                         & boggy                       & \texttt{qpcR}    \\
decision\_res\_C126EG595.csv                     & C126EG595                   & \texttt{chipPCR} \\
decision\_res\_C127EGHP.csv                      & C127EGHP                    & \texttt{chipPCR} \\
decision\_res\_C316.amp.csv                      & C316.amp                    & \texttt{chipPCR} \\
decision\_res\_C317.amp.csv                      & C317.amp                    & \texttt{chipPCR} \\
decision\_res\_C60.amp.csv                       & C60.amp                     & \texttt{chipPCR} \\
decision\_res\_CD74.csv                          & CD74                        & \texttt{chipPCR} \\
decision\_res\_competimer.csv                    & competimer                  & \texttt{qpcR}    \\
decision\_res\_dil4reps94.csv                    & dil4reps94                  & \texttt{qpcR}    \\
decision\_res\_guescini1.csv                     & guescini1                   & \texttt{qpcR}    \\
decision\_res\_guescini2.csv                     & guescini2                   & \texttt{qpcR}    \\
decision\_res\_htPCR.csv                         & htPCR                       & \texttt{qpcR}    \\
decision\_HCU32\_aggR.csv                        & HCU32\_aggR.csv             & \texttt{PCRedux} \\
decision\_res\_karlen1.csv                       & karlen1                     & \texttt{qpcR}    \\
decision\_res\_karlen2.csv                       & karlen2                     & \texttt{qpcR}    \\
decision\_res\_karlen3.csv                       & karlen3                     & \texttt{qpcR}    \\
decision\_res\_lievens1.csv                      & lievens1                    & \texttt{qpcR}    \\
decision\_res\_lievens2.csv                      & lievens2                    & \texttt{qpcR}    \\
decision\_res\_lievens3.csv                      & lievens3                    & \texttt{qpcR}    \\
decision\_res\_reps.csv                          & reps                        & \texttt{qpcR}    \\
decision\_res\_reps2.csv                         & reps2                       & \texttt{qpcR}    \\
decision\_res\_reps3.csv                         & reps3                       & \texttt{qpcR}    \\
decision\_res\_reps384.csv                       & reps384                     & \texttt{qpcR}    \\
decision\_res\_rutledge.csv                      & rutledge                    & \texttt{qpcR}    \\
decision\_res\_stepone\_std.csv                  & stepone\_std                & \texttt{RDML}    \\
decision\_res\_testdat.csv                       & testdat                     & \texttt{qpcR}    \\
decision\_res\_vermeulen1.csv                    & vermeulen1                  & \texttt{qpcR}    \\
decision\_res\_vermeulen2.csv                    & vermeulen2                  & \texttt{qpcR}    \\
decision\_res\_VIMCFX96\_60.csv                  & VIMCFX96\_60                & \texttt{chipPCR} \\ \hline
\end{tabular}
\end{table}


The following example shows the export of the `RAS002.rdml` file from the RDML format to the csv format.

```{r, echo=TRUE, eval=FALSE}
# Export the RDML data from the PCRedux package as the objects RAS002 and RAS003.
library(RDML)
library(PCRedux)
library(magrittr)
suppressMessages(library(data.table))

RAS002 <- data.frame(RDML$new(paste0(path.package("PCRedux"), 
                                     "/", "RAS002.rdml"))$GetFData())

# The obbject RAS002 can be stored in the working directory as CSV file with 
# the name RAS002_amp.csv.
write.csv(RAS002, "RAS002_amp.csv", row.names=FALSE)
```

### Amplification Curve Data in the `RDML` Format \label{section_Amplification_Curve_Data_in_the_RDML_Format}

Selected amplification cure data sets were stored in the RDML format as 
described in [@roediger2015r, @roediger_enabling_2017].

RDML data file           | Device    | Target gene           | Detection chemistry
-------------------------|-----------|-----------------------|------------------------------
RAS002.rdml              | CFX96     | HPV16, HPV18, HPRT1   | Taqman
RAS003.rdml              | CFX96     | HPV16, HPV18, HPRT1   | Taqman
hookreg.rdml             | Bio-Rad   | various               | Taqman, intercalating dyes

32HCU: VideoScan (Attomol GmbH), CFX96: Bio-Rad.

Table_human_rated.xlsx

\newpage

## Analysis of Simgmoid Shaped Curves - or the Analysis of Amplification Curve Data from Quantitative real-time PCR Experiments \label{section_introduction}

\texttt{PCRedux} is an \texttt{R} package that was developed for the analysis of sigmoid curves.  Sigmoid curves are common in many biological assays. A widely used bioanalytical method is the quantitative real-time PCR (qPCR). For example, qPCR are applied in human diagnostics and forensics  [@martins_dna_2015, @sauer_differentiation_2016]. qPCRs are performed in thermo-cyclers. There are numerous manufactures, which produce thermal cyclers as commercial products or as part of scientific projects. An example a thermal cycler form an scientific project is the VideoScan technology [@roediger_highly_2013].

A sigmoid function has a S-shaped curvature (\autoref{figure_sigmoid_cuve}). Sigmoid function are non-linear functions are real-valued and can be differentiated (first derivative maximum, with one local minimum and one local maximum). When analyzing qPCR data, it is of interest which information can be obtained from the form of the amplification curve. It does not matter whether it is a detection with gene specific probes or with an intercalating dye.

```{r, echo=FALSE, fig.cap="\\label{figure_sigmoid_cuve} Sigmoid curve.", fig.height=8}
x_val <- seq(-10, 10, 0.01)
y_val <- 1 / (1 + exp(-x_val))
y_val_slope <- 1 / (1 + exp(-x_val)) + 0.2
y_val_slope_quadratic <- 1 / (1 + exp(-x_val)) + 0.0005 * x_val ^ 2 + 0.2
y_val_slope_quadratic_noise <- 1 / (1 + exp(-x_val)) + 0.0005 * x_val ^ 2 + 0.2 + rnorm(length(x_val), mean = 0.01, sd = 0.05)

y_lim <- c(-0.05, max(c(y_val, y_val_slope, y_val_slope_quadratic, y_val_slope_quadratic_noise)) * 1.2)

# par(mfrow=c(2,2), las=0, bty="o", oma=c(1,1,1,1))
par(mfrow = c(2, 2), las = 0, bty = "o", oma = c(0, 0, 0, 0))

plot(x_val, y_val, type = "l", xlab = "x", ylab = "f(x)", ylim = y_lim)
abline(h = 0, col = "grey")
legend("topleft", expression(y == frac(1, (1 + e ^ {
  -x
}))), bty = "n", cex = 0.9)
mtext("A", cex = 1.5, side = 3, adj = 0, font = 2)

plot(x_val, y_val_slope, type = "l", xlab = "x", ylab = "f(x)", ylim = y_lim)
abline(h = 0, col = "grey")
legend("topleft", expression(y == frac(1, (1 + e ^ {
  -x
})) + n), bty = "n", cex = 0.9)
mtext("B", cex = 1.5, side = 3, adj = 0, font = 2)

plot(x_val, y_val_slope_quadratic, type = "l", xlab = "x", ylab = "f(x)", ylim = y_lim)
abline(h = 0, col = "grey")
legend("topleft", expression(y == frac(1, (1 + e ^ {
  -x
})) + m * x ^ 2 + n), bty = "n", cex = 0.9)
mtext("C", cex = 1.5, side = 3, adj = 0, font = 2)

plot(x_val, y_val_slope_quadratic_noise, type = "l", xlab = "x", ylab = "f(x)", ylim = y_lim)
abline(h = 0, col = "grey")
legend("topleft", expression(y == frac(1, (1 + e ^ {
  -x
})) + m * x ^ 2 + n + epsilon, epsilon %~% N(0, sigma)), bty = "n", cex = 0.9)
mtext("D", cex = 1.5, side = 3, adj = 0, font = 2)
```

The analysis of sigmoid data (e.g., quantitative PCR) it is a manageable task if the volume of data is low, or dedicated software is available for the analysis thereof. An example such a scenario (low number of amplification curves) is shown in \autoref{samples_of_qPCRs}A. All `r ncol(chipPCR::C127EGHP)-1` curves exhibit a sigmoid curve shape.

In contrast, the example in \autoref{samples_of_qPCRs}B is not manageable with a reasonable effort by simple visual inspection. The data originate from a high-throughput experiment encompasses in total `r suppressMessages(ncol(qpcR::htPCR))-1` amplification curves. Similarly, a manual analysis of the data is time-consuming and prone to errors.

```{r, echo=FALSE, fig.cap="\\label{samples_of_qPCRs}Amplification curve data from an iQ5 (Bio-Rad) thermal cycler and a high throughput experiment in the Biomark HD (Fluidigm). A) The `C127EGHP` data set (\\texttt{chipPCR} package, [@roediger2015chippcr]) with 64 amplification curves was produced in conventional thermal cycler with a 8 x 12 PCR grid. B) The `htPCR` data set (\\texttt{qpcR} package, [@Ritz2008]), which contains 8858 amplification curves, was produced in a 95 x 96 PCR grid. Only 2000 amplification curves are shown.", fig.height=5.5, fig.width=11}
options(warn = -1)
library(chipPCR)
library(qpcR)


# par(mfrow=c(1,2), las=0, bty="o", oma=c(1,1,1,1))
par(mfrow = c(1, 2), las = 0, bty = "o", oma = c(0, 0, 0, 0))
colors <- rainbow(ncol(C127EGHP) - 2, alpha = 0.3)
matplot(
  C127EGHP[, 2], C127EGHP[, c(-1, -2)], xlab = "Cycle", ylab = "RFU",
  main = "C127EGHP data set", type = "l", lty = 1, lwd = 2, col = colors
)
mtext("A", cex = 1.5, side = 3, adj = 0, font = 2)

colors <- rainbow(2000, alpha = 0.2)
matplot(
  htPCR[, 1], htPCR[, c(2L:2001)], xlab = "Cycle", ylab = "RFU",
  main = "htPCR data set", type = "l", lty = 1, lwd = 2, col = colors
)
mtext("B", cex = 1.5, side = 3, adj = 0, font = 2)
```


A challenge for the user is, to make sense from the vast amount of data produced. In particular, the reproducible and objective analysis of the amplification curve data exposes challenges to inexperienced users. Even among peers is not uncommon that they ``judge`` (classify) results differently. An example on this problem is given in \autoref{samples_of_qPCRs}.

Specialized software that can distinguish the amplification curves automatically is needed. There are several open source and closed source software tools, which can be used for the analysis of qPCR data [@pabinger_2014]. The software packages deal for example with

* missing values and non-detects [@mccall_non-detects_2014], 
* noise and artifact removal [@roediger2015chippcr, @roediger2015r, @spiess_impact_2015, @spiess_system-specific_2016],
* inter run calibration [@ruijter_removal_2015], 
* normalization [@roediger2015chippcr, @ruijter_evaluation_2013, @feuer_lemming:_2015, @matz_no_2013], 
* quantification cycle estimation [@Ritz2008, @ruijter_evaluation_2013], 
* amplification efficiency estimation [@Ritz2008, @ruijter_evaluation_2013], 
* data exchange [@lefever_rdml_2009, @perkins_readqpcr_2012, @roediger_enabling_2017],
* relative gene expression analysis [@dvinge_htqpcr:_2009, @pabinger_qpcr:_2009, @neve_unifiedwmwqpcr:_2014] and 
* data analysis pipelines [@pabinger_qpcr:_2009, @ronde_practical_2017, @mallona_pcrefficiency:_2011, @mallona_chainy:_nodate].

A large proportion of the algorithms is implemented in the \texttt{R} statistical computing language. However, more dedicated literature is available from peer-reviewed publications and textbooks.

It was shown that amplification curve analysis is not a simple task. For example selected qPCR systems (device and/or detection chemistry) cause periodicity in the amplification curve data [@spiess_system-specific_2016]. Or the commonly employed smoothing of data might case alterations to the raw data that affects both the estimation of the template material (Cq value) and the amplification efficiency [@spiess_impact_2015].

At this level all software assumes that the amplification resemble a sigmoid curve shape (ideal positive amplification reaction), or a flat low line (ideal negative amplification reaction). For example, @Ritz2008 published the \texttt{qpcR} \texttt{R} package that contains functions to fit several multi-parameter models. This includes the five-parameter Richardson function, which is often used for the analysis of qPCR data.

Most software packages do not take into account if an amplification curve fulfills a certain criterion. For example, they do not try to make a classification if an amplification curve is positive (good quality) or negative (bad quality). This is a needed information for some algorithms. For example, the \texttt{linreg} method by @ruijter_amplification_2009 requires a decision, if an amplification curve is positive or negative. 

To illustrate this problem an example for the analysis of amplification curves of the `htPCR` data set is given in \autoref{section_decision_modus}.

Researchers solved many challenges that were daunting the users of qPCR devices in the past. In particular, algorithms for the processing of the positive amplification curves are widely available. A bottleneck of qPCR data analysis is the lack of features that can be used to build classifiers for amplification curve data. A classifier herein refer to a vector of features that can be used to distinguish the amplification curves by their shape only.

One reason for this is the lack of features that are known for amplification curve data. Only few features for amplification curves are described in the literature. For example, tools described by us are the ``amptester()`` and the ``amptester.gui()`` functions, which are part of the \texttt{chipPCR} package [@roediger2015chippcr]. The \texttt{qpcR} package [@Ritz2008] contains an amplification curve test via the ``modlist()`` function. The parameter check="uni2" offers an analytical approach, as part of a method for the kinetic outlier detection. It checks for a sigmoid structure of the amplification curve. Then it tests for the location of the first derivative maximum and the second derivative maximum. However, multi-parameter functions, like sigmoid models, fit "successful" in most cases including noise and give false positive results.

## Principles of Amplification Curve Data Analysis \label{section_DataAnalysis}

**Note** that a large number of qPCR systems do not display the raw data of the amplification curves on the screen. Instead, the raw data is usually processed by the instrument software to remove fluorophore-specific effects and background noise. The ordinate often does not display absolute fluorescence, but rather the change in fluorescence per cycle. Smoothing algorithms may also have been used. When using the \texttt{PCRedux} package, it is therefore advisable to clarify beforehand which processing steps the amplification curves have been subjected to until data export. Failure to do so may result in misinterpretations and incorrect models.

The shape of a positive amplification curve follows in most cases a sigmoid shape \ref{amplification_curve_shapes}. The curvature of the amplification curve can be used as a quality measure. For example, fragmentation, inhibitors and sample handling errors during the extraction can be identified. The kinetic of fluoresce emission is proportional to the quantity of the synthesized DNA. Typical amplification curves have three phases. 

1. *Initial phase*: This phase occurs during the first cycles of the PCR. The fluorescence emission is in most cases flat. During the initial phase, only a weak fluorescence signal is generated that cannot be detected by the sensor system. This is often referred to as baseline or background signal. Fragmentation, inhibitors and sample handling errors would result in a prolonged initial phase. Apparently, there is only a phase shift or no signal at all. This is primarily due to the limited sensitivity of the instrument. Even in a perfect PCR reaction (double amplification per cycle), qPCR instruments cannot detect the fluorescence signal from the amplification. In these early cycles, the fluorescence signals only produce a fluorescence background signal. The PCR product signal is an insignificantly small component of the total signal. Nevertheless, this phase may indicate some typical properties. For example, the increase and signal variation can be characteristic of the qPCR system or probe system. In many instruments, this phase is used to determine the CT threshold (a statistically relevant increase outside the noise range). A signal that is far enough above this threshold is considered as coming from the amplicon. It is assumed that this early cycle phase is flat in the amplification curve. In some qPCR systems a flat amplification curve is expected in this phase. Slight deviations from this trend are presumed to be due to changes (e. g. disintegration of probes) in the fluorophores. Background correction algorithms are often used here to ensure that flat amplification curves without slope are generated. This can lead to errors and inevitably leads to a loss of information via the waveform of the raw data.
2. *Exponential phase*: This phase follows the initial phase and is also called *log phase*. This phase is characterized by a strong increase of the emitted fluorescence. In this phase, the amount doubles in each cycle under ideal conditions. The amount of the synthesized fluorescent labeled PCR product is high enough to be detected by the sensor system. This phase is used for the quantification. Fragmentation, inhibitors and sample handling errors would decrease the slop of the amplification curve.
3. *Plateau phase*: This phase follows the exponential phase. The cause for this lies in the exploitation of the limited resources (incl. primers, nucleotides, enzyme activity) in the reaction vessel. This limits the amplification reaction, so that the theoretical maximum amplification efficiency (doubling per cycle) no longer prevails. This turning point and the progressive limitation of resources finally leads to a plateau. In the plateau phase, there is sometime a signal decrease called *hook effect*.

If the amplification curve has only a slight gradient and no perceptible exponential phase, it can be assumed that the amplification reaction has failed. Causes may include poor specificity of the PCR primers, degraded sample material, degraded probes or detector problems. Such a curve can also occur if non-specific PCR products are created at different points in time. In this case, the superimposed signals can generate such a signal progression. If there is a lot of start DNA (detectable amplification in the first cycles) and the instrument software makes a background correction, amplification curves with a strongly negative trend can be erroneously generated.

Such phases can be roughly considered as regions of interest (ROI). As an example, the \textit{starting phase} is in the head area, while the \textit{plateau phase} is in the tail area. The \textit{exponential phase} is located between these two ROIs.


```{r, echo=FALSE, fig.cap="Graphical representation of qPCR amplification curves. The fluorescence emitted by the reporter dye (e.g, SYBR Green, EvaGreen) is plotted against cycle number. Data were taken from the `testdat` data set from the \\texttt{qpcR} package. \\label{amplification_curve_ROI}", fig.height=8, fig.width=8}
options(warn = -1)
library(qpcR)
library(PCRedux)

colors <- rainbow(10, alpha = 0.15)

x_range <- 1L:35
d <- testdat[x_range, ]
amp_data <- data.frame(
  d[, 1],
  pos = d[, 3] + 0.9,
  posReverse = (max(d[, 3]) - rev(d[, 3])) + 0.9,
  neg = d[, 4] + 0.9 + 0.0005 * d[, 1] ^ 2
)

# Calculation for the normal data
res_amp_data <- pcrfit(amp_data, 1, 2, l5)
res_takeoff <- takeoff(res_amp_data)


# Calculation for the reversed data
res_amp_data_reverse <- pcrfit(amp_data, 1, 3, l5)
res_takeoff_reverse <- takeoff(res_amp_data_reverse)
res_takeoff_reverse[[1]] <- nrow(d) - res_takeoff_reverse[[1]]
res_takeoff_reverse[[2]] <- amp_data[res_takeoff_reverse[[1]], 2] - res_takeoff_reverse[[2]] + min(amp_data[, 3])

exponentialRange <- c((res_takeoff[[1]] + 1):(res_takeoff_reverse[[1]] - 1))

backgroundplateu <- function(x) {
  bg <- mean(head(x, 10)) + 3 * sd(head(x, 10))
  plat <- mean(tail(x, 10)) - 3 * sd(tail(x, 10))
  list(bg = bg, plateau = plat)
}

res_lm <- lm(amp_data[exponentialRange, 2] ~ amp_data[exponentialRange, 1])

y_lim <- max(amp_data[, 2:4]) * 1.15

res_bgpl <- unlist(backgroundplateu(amp_data[, 2]))

layout(matrix(c(1, 1, 2, 3), 2, 2, byrow = TRUE), respect = TRUE)

plot(amp_data[, 1], amp_data[, 2], ylim = c(-0.1, y_lim), xlab = "Cycle", ylab = "RFU", type = "b", lwd = 2, pch = 19)

rect(0, 0, res_takeoff[[1]] + 1, res_takeoff[[2]] * 1.25, col = colors[1], border = NA)
text(5, res_bgpl[1] * 1.45, "Baseline region")

rect(res_takeoff_reverse[[1]] - 1, res_takeoff_reverse[[2]] * 0.95, nrow(amp_data), y_lim, col = colors[5], border = NA)
text(32, res_bgpl[2] * 1.1, "Plateau region")

text(res_takeoff_reverse[[1]], mean(amp_data[, 2]), "Exponential\nregion")


points(
  c(res_takeoff[[1]], res_takeoff_reverse[[1]]),
  c(res_takeoff[[2]], res_takeoff_reverse[[2]]), pch = 12, cex = 2.5
)

arrows(20, 0, 20, res_bgpl[1], code = 3, length = 0.1)
text(30, res_bgpl[1] / 2, "Background")

arrows(5, res_bgpl[2], 5, max(amp_data[, 2]), code = 3, length = 0.1)
text(15, res_bgpl[2] * 0.95, "Plateau")


abline(res_lm, col = "red")
points(amp_data[exponentialRange, 1], amp_data[exponentialRange, 2], pch = 19, col = "red")

abline(h = res_bgpl, col = c("green", "blue"))
abline(h = 0, col = "grey")

legend(2, 12, paste0(
  "Slope: ", signif(coef(res_lm)[2], 3),
  "\nBackground: ", signif(res_bgpl[1], 3),
  "\nPlateau: ", signif(res_bgpl[2], 3),
  "\nTOP: ", signif(res_takeoff[[1]], 3),
  "\nEND: ", signif(res_takeoff_reverse[[1]], 3)
), bty = "n")

mtext("A    Positive", cex = 1, side = 3, adj = 0, font = 2)



plot(amp_data[, 1], amp_data[, 4], ylim = c(-0.1, y_lim), xlab = "Cycle", ylab = "RFU", type = "b", lwd = 2, pch = 19)
res_bgpl <- unlist(backgroundplateu(amp_data[, 4]))
abline(h = res_bgpl, col = c("green", "blue"))
abline(h = 0, col = "grey")

mtext("B    Negative", cex = 1, side = 3, adj = 0, font = 2)

curve_colors <- c(rainbow(ncol(boggy) - 1, alpha = .5))
matplot(boggy[, 1], boggy[, -1], type = "l", col = curve_colors, xlab = "Cycle", ylab = "RFU", lty = 1)
mtext("C    boggy data set", cex = 1, side = 3, adj = 0, font = 2)
```


The most important measurement from qPCRs is the cycle of quantification (Cq), which signifies at which PCR cycle the fluorescence exceeds a ``threshold value``. There is an ongoing debate as to what a significant and robust threshold value is since there are several mathematical methods to calculate the Cq. The classical threshold value (cycle threshold, Ct) is a straight horizontal line, which intersects with the quasi-linear phase in the exponential amplification phase of the PCR. Another Cq method uses the maximum of second derivative (SDM)[@roediger2015r]. More details will be given in \autoref{}. A good overview and performance comparison is given in @ruijter_evaluation_2013.

In all cases the Cq can be used to calculate the concentration of target sequence in a sample (low Cq \textrightarrow high target concentration).

In contrast, negative or ambiguous amplification curves loosely resemble noise. This noise may appear linear or exhibit a curvature (Figure \ref{amplification_curve_shapes}). Many factors, such as the sample quality, qPCR chemistry, and technical problems (e.g., sensor errors) contribute to various curve shapes. A common phenomenon of amplification curve shapes is the 'hook effect' [@barratt_improving_2002, @ruijter_2014]. This however, may result in faulty interpretation of the amplification curves. Remarkable progress has been made in qPCR data analysis, primarily due to the availability of sophisticated data analysis pipelines (e.g., @ruijter_evaluation_2013, @ruijter_removal_2015, @roediger2015r, @spiess_impact_2015, @spiess_system-specific_2016). However, the availability of comprehensively annotated data sets of amplification curves remains a challenge.


```{r, echo=FALSE, fig.cap="Amplification curve shapes. \\label{amplification_curve_shapes}", fig.height=3}
options(warn = -1)
library(qpcR)
library(PCRedux)
library(data.table)
filename <- system.file("decision_res_competimer.csv", package = "PCRedux")
res_competimer <- fread(filename, data.table = FALSE)

curve_colors <- c(rainbow(ncol(competimer) - 1, alpha = .5))

y_lim <- c(0, max(competimer[, -1]))

par(mfrow = c(1, 3))
matplot(competimer[, 1], competimer[, colnames(competimer) %in% res_competimer[res_competimer[["test.result.1"]] == "y", ]$competimer], type = "l", col = curve_colors, xlab = "Cycle", ylab = "RFU", lty = 1, ylim = y_lim)
matplot(competimer[, 1], competimer[, colnames(competimer) %in% res_competimer[res_competimer[["test.result.1"]] == "a", ]$competimer], type = "l", col = curve_colors, xlab = "Cycle", ylab = "RFU", lty = 1, ylim = y_lim)
matplot(competimer[, 1], competimer[, colnames(competimer) %in% res_competimer[res_competimer[["test.result.1"]] == "n", ]$competimer], type = "l", col = curve_colors, xlab = "Cycle", ylab = "RFU", lty = 1, ylim = y_lim)
```

This means that the amplification curve shape, the amplification efficiency and the Cq value are essential to judge the outcome of a qPCR reaction. In all phases of PCR the curves should be smooth. Possible peaks in the curves may be due to unstable light sources from the instrument or problems during sample preparation, such as the presence of bubbles in the reaction vessel.

An important step in the qPCR workflow is data analysis. At this stage amplification curve [@roediger2015chippcr] and melting curve pre-processing [@roediger_RJ_2013] needs to be performed to continue next with the feature extraction from the curvatures. Several studies have been done, which discuss the pre-processing and post-processing of qPCR data [@roediger2015chippcr, @spiess_impact_2015, @spiess_system-specific_2016]. In this work the focus is on amplification curve data. Amplification curves can be difficult to interpret and analyze if the curvature deviates from the ideal sigmoid shape, or the volume of curve data is to large for an economic manual analysis. Moreover, amplification curves may look acceptable for an inexperienced use but unacceptable for an expert. Therefore, there is a need for method of statistical interrogation and objective interpretation of results.

The quantification of nucleic acids by curve parameters like the quantification point (Cq) and the amplification efficiency (AE) is only meaningful if the kinetic of the amplification curve follows a sigmoid structure according to the model the qPCR [@ruijter_evaluation_2013, @ruijter_2014, @Ritz2008]. In qPCRs is sigmoid shape is characterized by a baseline region, an exponential region and a (maximum) plateau phase. The magnitude of the raw fluorescence and the shape of the amplification curve vary naturally between detection probe systems and devices. Therefore, it is challenging identifying negative curves which appear to be positive but just an artifact of scaling.

Most assays have an intrinsic property, which can be used to decide if an amplification reaction is positive, negative or ambiguous. Melting curve analysis belongs to the commonly used approaches [@roediger_RJ_2013]. For example qPCRs monitored with unspecific dyes (e.g., EvaGreen) use melting curve analysis is a post-processing method to identify positive samples. Some detection probe systems like hydrolysis probes do not permit such methods.

For examples the \texttt{linreg} method by @ruijter_amplification_2009 require a decision, which distinguishes between positive and negative reaction. 

A typical situation is that results samples may positive, negative or ambiguous. The later are most problematic because both outcomes (positive and negative) might be true. However, in most cases the user is interested in an automatic distinction between positive and negative samples. This is import in screening applications.

A threshold based method is a simple yet effective approach. This method requires that amplification curves are properly base-lined prior to the analysis. This is not always desirable. Methods of ``amptester()`` function are more sophisticated and are targeted at two user groups. The first covers developers who wish to implement their own routines. The second covers users who wish to include an initial quality control.

Provided that the rules are strict and transparent, such a routine can be used for quality management. This is also in conformance with the philosophy that software in research and diagnostics should be a foundation for reproducible research [@roediger2015r].

The Ct method appears to be the most widely used method despite the fact that this method was shown to be unreliable [@ruijter_evaluation_2013, @spiess_impact_2015, @spiess_system-specific_2016]. Presumably this due to the familiarity of users with this approach since it is also known from chemical analysis procedures or basic calculus.  Another reason might be that the Ct method is easy to implement and to understand. The calculation is strongly dependent on the user, who has to adjust the threshold level manually. Thus, the Ct method is not stable in predictions if several users are given the same data set to be analyzed. Moreover, the Ct method makes the assumption that the amplification efficiency (~ slope in the log-linear phase) is equal across all amplification curves compared [@ruijter_evaluation_2013]. Evidently, this is not always case as exemplified in \autoref{amplification_curve_ROI}C.

Another approach is to use non-linear model to fit the amplification curve. For example, 5-parameter Richardson functions (equation \ref{eq_Richards}, @richards_flexible_1959) are often used [@spiess_impact_2015].

\begin{equation}
\label{eq_Richards}
y=c + \frac{d - c}{(1 + exp(b(log(x) - log(e))))^f}
\end{equation}


```{r, echo=FALSE, results='hide',message=FALSE, fig.cap="Commonly used methods for the analysis of quantification points. A) Linear plot of an amplification curve with a typical sigmoid shape. The Grey horizontal line is the threshold as determined by the \\textit{68-95-99.7 rule} from the fluoresce emission of cycle 1 to 10. The red horizontal line is the user defined threshold in the log-linear range of the amplification curve. The Ct is calculated from the intersection of the horizontal line and a quadratic polynomial fitted in to the amplification curve (see @roediger2015chippcr for details). B) The Amplification curve plot with a logarithmic ordinate visualizes the linear phase. C) Analysis of the amplification curve by fitting with a five parameter model (Equation \\ref{eq_Richards}). \\label{figure_quntifcation_points}", fig.height=8, fig.width=8} 
options(warn = -1)
library(qpcR)
library(chipPCR)
library(magrittr)

res_model <- pcrfit(testdat, cyc = 1, fluo = 2, model = l5)
res_takeoff <- takeoff(res_model, pval = 0.05, nsig = 3)
res_th.cyc <- th.cyc(testdat[, 1], testdat[, 2], r = 2.356, linear = FALSE)


# par(las=0, oma=c(0.5, 0.5, 0.5, 0.5))
par(las = 0, oma = c(0, 0, 0, 0))

layout(matrix(c(1, 2, 3, 3), 2, 2, byrow = TRUE), respect = TRUE)


plot(testdat[, 1], testdat[, 2], xlab = "Cycles", ylab = "Raw fluorescence")

abline(h = (mean(testdat[1:10, 2]) + 3 * sd(testdat[1:10, 2])), col = "grey")

abline(h = res_th.cyc[1, 2], col = "red")
arrows(res_th.cyc[1, 1], res_th.cyc[1, 2], res_th.cyc[1, 1], 0, angle = 25, length = 0.1, lwd = 2)
legend("bottomright", paste0("Ct=", signif(res_th.cyc[1, 1], 4)), bty = "n")
mtext("A", cex = 1.25, side = 3, adj = 0, font = 2)

plot(testdat[, 1], log(testdat[, 2]), xlab = "Cycles", ylab = "log(Raw fluorescence)")
abline(h = log(res_th.cyc[1, 2]), col = "red")
arrows(res_th.cyc[1, 1], log(res_th.cyc[1, 2]), res_th.cyc[1, 1], min(log(testdat[, 2]), na.rm = TRUE), angle = 25, length = 0.1, lwd = 2)
legend("bottomright", paste0("Ct=", signif(res_th.cyc[1, 1], 4)), bty = "n")
mtext("B", cex = 1.25, side = 3, adj = 0, font = 2)

res_efficiency <- efficiency(res_model)
arrows(res_takeoff[[1]], res_takeoff[[2]], res_takeoff[[1]], -0.2, angle = 25, length = 0.1, lwd = 2)
mtext("C", cex = 1.25, side = 3, adj = 0, font = 2)
```

> **A comment on noise in sigmoid amplification cures**: Noise in amplification curves can have very different causes. Among them are incorrectly assigned dye detectors, errors during the calibration of dyes for the instrument, errors during the preparation of the PCR master mix, sample degradation, lack of a sample in the PCR, too much sample material in the PCR mix or a low detection probe concentration.

\newpage

## Functions of the \texttt{PCRedux} Package \label{section_Functions_of_PCRedux}

There are a number of ROIs (see figure \autoref{amplification_curve_ROI}) in an amplification curve that are potentially useful for calculating characteristics for the classification of amplification curves. As a rule, amplification curves deviate differently from the ideal sigmoid models (compare figure \autoref{figure_sigmoid_cuve} and figure \autoref{samples_of_qPCRs}) of qPCRs. For instance, some amplification curves are only flat or have a rise with positive or negative signs without sigmodic function. In the case of sigmoid amplification curves, there are turning points which can be characteristic for positive amplification curves. Such differences are interesting candidates to calculate features for machine learning. On the basis of this observation, various concepts were developed and implemented in algorithms to describe amplification curves.

The intent of @gunay_machine_2016 was to improve the Cq value determinations. They postulated that they can achieve an improved prediction of Cq values using a modified sigmoid function (three parameters). However, this is questionable. Because a basic assumption, from their approach, was that this model can be applied to any data set. There are several reasons why such an assumption is not universally valid. In the chapters \autoref{section_hookreg}, for example, the functions ``hookreg()`` and ``hookregNL()`` are briefly displayed. These amplification curves deviate significantly from a three-parameter model. In addition, non-linear functions also tend to adjust noise (\autoref{curve_fit_fail}). Therefore, several characteristics of an amplification curve should be recorded first and then checked for their usefulness. There, it becomes clear that a three-parameter model adaptation can be adapted to noise and thus provides unreliable predictions. 

As shown in \autoref{amplification_curve_shapes} have positive amplification curves a typical sigmoid shape, while negative curves resemble random noise.

Many properties (e.g., experiment condition (hydrolysis probe, intercalating dye)) can be converted to binary classifiers (no == 0, yes == 1). From the amplification curve one can calculate the signal range before and after the amplification process.

Next follows a brief introduction of the feature engineering process of this study. For doing this a set features which characterize amplification curves was needed. In total seven function were generated and integrated in \texttt{PCRedux} package. These functions features have not been described before in the literature for the classification of amplification curves.

The function described following are aimed for experimental studies. It is important to note that the features proposed herein emerged during a critical reasoning process. The aim of the package is to propose a set of features, functions and data for an independent research.

### ``autocorrelation_test()`` - A Function to Detect Positive Amplification Curves \label{section_autocorrelation_test}

Autocorrelation analysis is a technique that is used in the field of time series analysis. It can be used to reveal regularly occurring patterns in one-dimensional data [@spiess_system-specific_2016]. The autocorrelation measures the correlation of a signal $f(t)$ with itself shifted by some time delay $f(t - \tau)$.

The ``autocorrelation_test()`` function coercers the amplification curve data to an object of the class "zoo" (\texttt{zoo} package) as indexed totally ordered observations. Next follows the computation of a lagged version of the amplification curve data. The shifting the amplification curve data is based back by a given number of observations (default $\tau=12$).

```{r, echo=TRUE, fig.cap="\\label{figure_autocorrelation_tau} Effect of tau.", fig.height=8}
library(PCRedux)
library(RDML)

amp_data <- data.frame(RDML$new(paste0(path.package("PCRedux"), "/", 
                                       "RAS002.rdml"))$GetFData())

tau <- seq(1,25,1)

par(mfrow=c(2,2))

plot(amp_data[, 1], amp_data[, 10], xlab = "Cycle", ylab = "RFU", pch=19)
mtext("A", cex = 1.5, side = 3, adj = 0, font = 2)

ac_pos <- sapply(1L:length(tau), function(i) {
      autocorrelation_test(amp_data[, 2], n=tau[i], ns_2_numeric=TRUE)
  })

plot(tau, ac_pos, ylim=c(0,1), pch=19)
points(tau[ac_pos == min(ac_pos)], ac_pos[ac_pos == min(ac_pos)], pch=1, cex=2, 
       col="red")
legend("bottomleft", paste("tau: ",  tau[ac_pos == min(ac_pos)],
                           "\nAC: ", signif(ac_pos[ac_pos == min(ac_pos)])), 
       bty="n")
mtext("B", cex = 1.5, side = 3, adj = 0, font = 2)

plot(amp_data[, 1], amp_data[, 15], xlab = "Cycle", ylab = "RFU", pch=19)
mtext("C", cex = 1.5, side = 3, adj = 0, font = 2)

ac_neg <- sapply(1L:length(tau), function(i) {
    autocorrelation_test(amp_data[, 15], n=tau[i])
})

plot(tau, ac_neg, ylim=c(0,1), pch=19)

mtext("D", cex = 1.5, side = 3, adj = 0, font = 2)
```

Then follows a significance test for correlation between paired samples (amplification curve data & lagged amplification curve data). The hypothesis is that the paired sample of positive amplification curves has a significant correlation (`stats::cor.test`, significance level is 0.01) in contrast to negative amplification curves (noise). The application of the ``autocorrelation_test()`` function is shown in the following example.

In addition, the the decisions file `decision_res_RAS002.csv` from the human expert was analyzed for the most frequent decision (modus) using the ``decision_modus()`` function (\autoref{section_decision_modus}).

```{r, echo=TRUE, fig.cap="\\label{autocorrelation}Autocorrelation analysis for amplification curves of the `RAS002` data set (\\texttt{PCRedux} package). A) Plot of all amplification curves of the `RAS002` data set. B) Density plot of B) Positive curves and negative curves as determined by the `autocorrelation_test()` and a human expert. C) Performance analysis by the ``performeR()`` function (see \\autoref{section_performeR} for details).", fig.height=5.5, fig.width=11}
# Test for autocorrelation in amplification curve data
# Load the libraries magrittr for pipes and the amplification curve the data
# The amplification curve data from the `RAS002` data set
# was imported from the RDML format.
# The data.table package was used for fast import of the csv data
options(warn = -1)
library(magrittr)
library(PCRedux)
suppressMessages(library(data.table))

RAS002 <- data.frame(RDML$new(paste0(path.package("PCRedux"), "/", "RAS002.rdml"))$GetFData())

# Test for autocorrelation in the RAS002 data set

res_ac <- sapply(2:ncol(RAS002), function(i) {
    autocorrelation_test(RAS002[, i], ns_2_numeric=TRUE)
})

# Curves classified by a human after analysis of the overview. 1=positive,
# 0=negative

human_classification <- fread(system.file("decision_res_RAS002.csv", 
                                          package = "PCRedux"))

head(human_classification)


decs <- sapply(1L:nrow(human_classification), function(i) {
    res <- decision_modus(human_classification[i, 2L:(ncol(human_classification)-1)])
    if(length(res) > 1) res[[1]] <- "n"
        res[[1]]
}) %>% unlist()


# Plot curve data as overview
# Names of the samples

layout(matrix(c(1, 2, 3, 1, 4, 4), 2, 3, byrow = TRUE))
matplot(
  RAS002[, 1], RAS002[, -1], xlab = "Cycle", ylab = "RFU",
  main = "RAS002 and RAS003 data sets", type = "l", lty = 1, 
  col = decs, lwd = 2
)
legend("topleft", c("positive", "negative"), pch = 19, col = c(1,2), bty = "n")
mtext("A", cex = 1.5, side = 3, adj = 0, font = 2)


# Convert the n.s. (not significant) in 0 and others to 1.
# Combine the results of the aromatic autocorrelation_test as variable "ac",
# the human classified values as variable "hc" in a new data frame (res_ac_hc).

cutoff <- 0.8

res_ac_hc <- data.frame(
  ac = ifelse(res_ac > cutoff, 1, 0),
  hc = as.numeric(as.factor(decs))-1
) %>% as.matrix()
res_performeR <- performeR(res_ac_hc[, "ac"], res_ac_hc[, "hc"])


plot(density(res_ac), ylab = "Autocorrelation", main = "")
rug(res_ac)

abline(v = cutoff)
mtext("B", cex = 1.5, side = 3, adj = 0, font = 2, las = 0)


cdplot(as.factor(decs) ~ res_ac, xlab="Autocorrelation", 
       ylab="Decision")
mtext("C", cex = 1.5, side = 3, adj = 0, font = 2, las = 0)

barplot(
    as.matrix(res_performeR[, c(1:10, 12)]), yaxt = "n", ylab = "",
        main = "Performance of autocorrelation_test"
)

axis(2, at = c(0, 1), labels = c("0", "1"), las = 2)
mtext("D", cex = 1.5, side = 3, adj = 0, font = 2, las = 0)
```

As shown in this example, the ``autocorrelation_test()`` function is able to distinguish between positive and negative amplification curves. Negative amplification curve were in all cases non-significant. In contrast, the coefficients of correlation for positive amplification curves ranged between `r signif(range(as.numeric(res_ac), na.rm=TRUE), 3)[1]` and `r signif(range(as.numeric(res_ac), na.rm=TRUE), 3)[2]` at a significance level of 0.01 and a lag of 3.

### ``decision_modus()`` - A Function to Get a Decision (Modus) from a Vector of Classes \label{section_decision_modus}

Many approaches to machine learning exist. The subject is very rich. One method is supervised machine learning, where the goal is to derive a property from user-defined (classified) training data. Classified training data can be created by one or more individuals, for example. Categories such as negative, ambiguous or positive are assigned depending on the form of the amplification curve, similar to what was described in \autoref{section_introduction} and \autoref{section_DataAnalysis}. 

For example, the amplification curves in (\autoref{htPCR_nap}) were taken from the `htPCR` data set (see \autoref{samples_of_qPCRs}B). Assuming that the classification of the amplification curves is delegated to different users, it is likely that the amplification curve (`r colnames (htPCR)[512]`, \autoref{htPCR_nap}) will be considered ambiguous or even positive ($\leftrightarrow$ positive ambivalent) by the users. A classification experiment was carried out for the complete `htPCR` data set. For this purpose, the amplification curves were classified at different time points as described in @roediger2015chippcr.

\autoref{tableheaddecision} shows from a total of `r ncol(qpcR:: htPCR)-1` amplification curves the first 25 lines classified as negative (*conformity=TRUE*) and the first 25 lines classified as positive.  In total, the curves were classified eight times (`Test Score. 1` $\ldots$ `Test Score. 8`), resulting in a whole of 70864 individually analyzed amplification curves for this data set. All the raw data is included in the CSV file. 

```{r, eval=TRUE, echo=TRUE, results='asis'}
options(warn = -1)
library(PCRedux)
library(xtable)
library(data.table)
library(magrittr)

filename <- system.file("decision_res_htPCR.csv", package = "PCRedux")
decision_res_htPCR <- fread(filename, data.table = FALSE)

print(
  xtable(
    rbind(
      head(subset(decision_res_htPCR, conformity == FALSE), 25),
      head(subset(decision_res_htPCR, conformity == TRUE), 25)
    ), caption = ".",
    label = "tableheaddecision"
  ), include.rownames = FALSE, caption.placement = "top",
  size = "\\tiny", comment = FALSE
)
```

This example shows that the amplification curves have been classified differently in `r paste0(signif(sum(decision_res_htPCR$conformity == FALSE) / nrow(decision_res_htPCR) * 100, 3), "%")` of the cases (e.g., line 1 "P01. W01"). While for other amplification curves all classifications were the same (e.g., line 8856 "P95. W94").

For the systematic statistical analysis of classification data sets, the ``decision_modus()`` function has been developed. This allows the most common decision (mode) to be determined. This feature is useful if you want to consolidate large collections of different decisions into a single decision. 

Observed:"a", "a", "a", "a", "a", "n", "n", "n" $\rightarrow$ frequencies 5 x "a", 3 x "n" $\rightarrow$ mode:"a"

Since the class names are known, they only have to be interpreted by the user (e.g., "a","n","y" -> "ambivalent","negative","positive"). 

The ``decision_modus()`` function was applied to the record `decision_res_htPCR.csv` with all classification rounds (columns 2 to 9) and the mode was determined for each amplitude curve.

```{r, eval=TRUE, echo=TRUE}
# Use decision_modus() to go through each row of all classification done by
# a human.

dec <- lapply(1L:nrow(decision_res_htPCR), function(i) {
  decision_modus(decision_res_htPCR[i, 2:9])
}) %>% unlist()

names(dec) <- decision_res_htPCR[, 1]

# Show statistic of the decisions
summary(dec)
```

```{r, echo=FALSE, fig.cap="\\label{htPCR_nap}A) Comparison of amplification curves. Examples of a negative (black), ambiguous (red) and positive (green) amplification curve were selected from the `htPCR` data set (\\texttt{qpcR} package, @Ritz2008). The negative amplification curve is not sigmoid and shows a strong positive trend. The ambiguous amplification curve approaches a sigmoid from, but shows a positive slope in the background (cycle 1 $\\rightarrow$ 5). The positive amplification curve is sigmoid. It begins in the background phase (cycle 1 $\\rightarrow$ 5) with a flat baseline, and shortly thereafter the exponential phase follows (cycle 5 $\\rightarrow$ 25) followed by a plateau phase (cycle 26 $\\rightarrow$ 35). B) Summary of frequencies of all classes of the `htPCR` record. negative, black; ambiguous, red; positive, green.", fig.height=5.5, fig.width=11}
options(warn = -1)
par(mfrow = c(1, 2))

library(qpcR)
matplot(
  htPCR[, 1], htPCR[, c(552, 512, 616)], xlab = "Cycle", ylab = "RFU",
  main = "htPCR data set", type = "l", lty = 1, lwd = 2
)
legend("topleft", c(
  paste("negative ", colnames(htPCR)[552]),
  paste("ambiguos ", colnames(htPCR)[512]),
  paste("positive ", colnames(htPCR)[616])
), col = 1:3, pch = 19, bty = "n")
mtext("A", cex = 1.5, side = 3, adj = 0, font = 2, las = 0)

# Plot the Frequencies of the decisions
barplot(
  table(dec), xlab = "Decision", ylab = "Frequency",
  main = "", col = c(2, 3, 1)
)
mtext("B    Classified by human", side = 3, adj = 0, font = 2, las = 0)
```

Another usage mode ``decision_modus()`` function is to set the parameter `max_freq=FALSE`. That option specifies the number of all classifications.

```{r, eval=TRUE, echo=TRUE}
library(PCRedux)
# Decisions for sample P01.W06
res_dec_P01.W06 <- decision_modus(decision_res_htPCR[
    which(decision_res_htPCR[["htPCR"]] == "P01.W06"),
  2L:9
], max_freq = FALSE)
print(res_dec_P01.W06)
```

This amplification curve `P01. W06` was classified as `r paste0(res_dec_P01.W06$variable[1], "=", res_dec_P01.W06$freq[1])` times and as `r paste0(res_dec_P01.W06$variable[2], "=", res_dec_P01.W06$freq[2])` times. Therefore, the decision would turn into a `negative` decision.

### ``earlyreg()`` - A Function to Calculate the Slope and Intercept of Amplification Curve Data from a qPCR Experiment \label{section_earlyreg}

The use of the slope and the intercept is suggested here to distinguish the amplification curves. For this purpose, the function ``earlyreg()`` was developed. This wrapper function uses an MM-type estimator [@todorov_object-oriented_2009] for robust linear regression within a limited number of cycles. As ROI, the first six cycles were defined. This restriction is based on empirical data suggesting that during the first six cycles only a significant increase in signal strength can be measured with a few qPCRs. However, ``earlyreg()`` ignores the first cycle, as many thermal cyclers use this cycle for sensor calibration. Extreme values are therefore limited from the very beginning. As standard, the next six amplitude values are used for robust linear regression. The number of cycles can also be adjusted via the parameter *range*.

The correlation coefficient is a measure to quantify the dependence on variables (e. g. number of cycles, signal height). The correlation coefficient is always between -1 and 1, with a value close to -1 describing a strong-negative dependency and close to 1 describing a strong-positive dependency; if the value is 0, there is no dependency between the variables. The most frequently used correlation coefficient to describe a linear dependency is the Pearson correlation coefficient *r*.
The correlation coefficient can also be used as a feature. Because similar data structures will have similar correlation coefficients. 
Correlation coefficients are usually between -1 and +1, with -1 being a strong negative correlation and 1 a strong positive correlation. The values of -1 and 1 have a perfect correlation. If the value is 0, there is no correlation between the two variables. However. variables that are not strongly correlated can also be important for modeling.


```{r, echo=TRUE, fig.cap="\\label{curve_fit_fail}Sample data from the `testdat` data set (\\texttt{qpcR} package, [@Ritz2008]) with negative and positive amplification curves.", fig.height=7, fig.width=7,eval=FALSE}
# Load the drc package for the five-parameter log-logistic fit.
library(drc)
library(chipPCR)

# Load the testdat data set from the qpcR package without
# loading the entire package.
data(testdat, package = "qpcR")

# Arrange graphs in an orthogonal matrix and set the plot parameters.
par(mfrow = c(2, 2))

# Apply the the amptester function to all amplification curve data
# and write the results to the main of the plots.

curve_data_columns <- c(2, 4, 9, 20)

for (i in 1L:length(curve_data_columns)) {
  res.ampt <- suppressMessages(amptester(testdat[, curve_data_columns[i]]))

  # Make a logical connection by two tests (shap.noisy, lrt.test and
  # tht.dec) of amptester to decide if an amplification reaction is
  # positive or negative.
  decision <- ifelse(!res.ampt@decisions[1] &&
    res.ampt@decisions[2] &&
    res.ampt@decisions[4],
  "positive", "negative"
  )

  plot(
    testdat[, 1], testdat[, curve_data_columns[i]],
    xlab = "Cycle", ylab = "RFU", pch = 19, main = ""
  )
  mtext(LETTERS[i], cex = 2, side = 3, adj = 0, font = 2)
  legend(
    "bottomright", paste0(
      colnames(testdat)[curve_data_columns[i]],
      "\nDecision: ", decision
    ),
    bty = "n", cex = 1.25, col = "red"
  )
  # Use the drm function with a five-parameter log-logistic fit model.

  try(lines(predict(drm(
    testdat[, curve_data_columns[i]] ~ testdat[, 1],
    fct = LL.3()
  )), col = 2), silent = TRUE)

  try(lines(predict(drm(
    testdat[, curve_data_columns[i]] ~ testdat[, 1],
    fct = LL.4()
  )), col = 3), silent = TRUE)
}
```

The following example illustrates a possible use of the function ``earlyreg()``. For that purpose 500 amplification curves were randomly drawn from the `htPCR` data set and used for analysis. In figure \autoref{earlyreg_slopes}A the amplification curves for all cycles are shown. The figure \autoref{earlyreg_slopes}B shows only the first ten cycles.


```{r, echo=TRUE, fig.cap="\\label{earlyreg_slopes}Amplification curves from the `htPCR` data set (\\texttt{qpcR} package). The amplification curves show different slopes in the early phase (cycle 1 to 10) of the qPCR.", fig.height=5.5, fig.width=11}
options(warn = -1)
library(qpcR)

par(
  bty = "o", font = 2, font.axis = 2, las = 1, cex.axis = 1.3, cex.lab = 1.3, lwd = 1.3,
  #     bg="white", oma=c(.5,.5,.5,.5))
  bg = "white", oma = c(0, 0, 0, 0)
)
data <- htPCR[, 1:501]
curve_colors <- c(rainbow(ncol(data) - 1, alpha = .5))
range <- 1:10

# par(mfrow=c(1,2), las=0, bty="o", oma=c(1,1,1,1))
par(mfrow = c(1, 2), las = 0, bty = "o", oma = c(0, 0, 0, 0))

matplot(
  data[, 1], data[, -1], col = curve_colors, pch = 19, lty = 1, type = "l",
  xlab = "Cycle", ylab = "RFU", main = "htPCR data set"
)
mtext("A", cex = 1.5, side = 3, adj = 0, font = 2)

matplot(
  data[range, 1], data[range, -1], col = curve_colors, pch = 19, lty = 1,
  type = "l", xlab = "Cycle", ylab = "RFU",
  main = "htPCR data set\n Cycle 1 to 10"
)
mtext("B", cex = 1.5, side = 3, adj = 0, font = 2)
```

Next, the ``earlyreg()`` function was used to determine the slope and intercept in the range of the first six PCR cycles. For the further analyses, only slope was used, because in this case  the intercept merely describes the background fluorescence within this data set. When additional data sets are included, the offset by the zero point should be used as well. The results were used in a cluster analysis using k-means clustering. As shown in Figure \autoref{earlyreg_results}B, the density function from the values of the zero shift resembles a monomodal bell curve. By contrast, the density function from the rises is rather bimodal (\autoref{earlyreg_results}C). Therefore, the increase seems to be an indicator of differences between the amplification curves.

When analyzing clusters with k-means (\textit{k}3 it was assumed that some amplification curves have a small, moderate or strong increase. After the cluster analysis this could also be observed (\autoref{earlyreg_results}D-F). Hence, it can be postulated that the increase in the background phase is helpful for the classification of amplification curves.

```{r, echo=TRUE, fig.cap="\\label{earlyreg_results}Clusters of samples according to their slope and intercept. The `htPCR` data set (\\texttt{qpcR} package) was used. The amplification curves show different slopes in the early phase (cycle 1 to 10) of the qPCR. Both the slope and the intercept were used for a cluster analysis (k-means).", fig.height=10, fig.width=10} 

# Normalize each amplification curve to their 0.99 percentile and use the
# earlyreg function to determine the slope and intercept of the first
# 6 cycles
options(warn = -1)
library(qpcR)

data <- htPCR[, 1:501]

res_slope_intercept <- do.call(rbind, lapply(2L:ncol(data), function(i) {
  earlyreg(x = data[, 1], y = data[, i], range = 7, normalize = TRUE)
}))
# Label the sample with their original names
rownames(res_slope_intercept) <- colnames(htPCR)[2:ncol(data)]
# Use k-means for cluster analysis
res_kmeans <- kmeans(res_slope_intercept[, "slope"], 3)$"cluster"

par(mfrow = c(2, 3))
plot(res_slope_intercept, col = res_kmeans)
mtext("A", cex = 1.5, side = 3, adj = 0, font = 2)

plot(density(res_slope_intercept[, "intercept"]), main = "Intercept")
mtext("B", cex = 1.5, side = 3, adj = 0, font = 2)

plot(density(res_slope_intercept[, "slope"]), main = "Slope")

for (i in unique(res_kmeans)) {
  abline(v = min(res_slope_intercept[res_kmeans == i, "slope"]), col = i)
}
mtext("C", cex = 1.5, side = 3, adj = 0, font = 2)

matplot(
  data[, 1], data[, which(res_kmeans == 1) + 1], col = curve_colors, pch = 19,
  lty = 1, type = "l", xlab = "Cycle", ylab = "RFU",
  main = "htPCR data set (1-500)\n Cluster 1", ylim = c(min(
    data[, -1],
    na.rm = TRUE
  ), 0.4)
)
mtext("D", cex = 1.5, side = 3, adj = 0, font = 2)

matplot(
  data[, 1], data[, which(res_kmeans == 2) + 1], col = curve_colors, pch = 19,
  lty = 1, type = "l", xlab = "Cycle", ylab = "RFU",
  main = "htPCR data set (1-500)\n Cluster 2", ylim = c(min(
    data[, -1],
    na.rm = TRUE
  ), 0.4)
)
mtext("E", cex = 1.5, side = 3, adj = 0, font = 2)

matplot(
  data[, 1], data[, which(res_kmeans == 3) + 1], col = curve_colors, pch = 19,
  lty = 1, type = "l", xlab = "Cycle", ylab = "RFU",
  main = "htPCR data set (1-500)\n Cluster 3", ylim = c(min(
    data[, -1],
    na.rm = TRUE
  ), 0.4)
)
mtext("F", cex = 1.5, side = 3, adj = 0, font = 2)
```

### ``head2tailratio()`` - A Function to Calculate the Ratio of the Head and the Tail of a Quantitative PCR Amplification Curve\label{section_head2tailratio}


The ``head2tailratio()`` function calculates the ratio of the head and the tail of a quantitative PCR amplification curve. Positive amplification curves have different slopes and intercepts at the start of the amplification curve (head, background region) and the end of the amplification curve (tail, plateau region). Therefore, these segments are potentially useful to extract a feature for an amplification curve classification.

```{r, echo=TRUE, fig.cap="\\label{dil4reps94_head2tailratio}Calculation of the ratio between the head and the tail of a quantitative PCR amplification curve. A) Plot of all amplification curves from the `dil4reps94` data set. B) For amplification curves from each dilution (1:1, red;  1:10, green, 1:100, cyan; 1:1000, purple) of the `dil4reps94` data set were selected to visualize the approach of the head2tailratio() function.", fig.height=5.5, fig.width=11}
options(warn = -1)
library(qpcR)

data <- dil4reps94[, c(1, 2, 98, 194, 290)]

res_head2tailratio <- lapply(2L:ncol(data), function(i) {
  head2tailratio(
    y = data[, i], normalize = TRUE, slope_normalizer = TRUE,
    verbose = TRUE
  )
})

data_normalized <- cbind(
  data[, 1],
  sapply(2L:ncol(data), function(i) {
    data[, i] / quantile(data[, i], 0.999)
  })
)

# par(mfrow=c(1,2), las=0, bty="o", oma=c(1,1,1,1))
par(mfrow = c(1, 2), las = 0, bty = "o", oma = c(0, 0, 0, 0))

colors <- c(
    rep("#FF00004D", 94),
    rep("#80FF004D", 94),
    rep("#00FFFF4D", 94),
    rep("#8000FF4D", 94)
)

matplot(
    dil4reps94[, 1], dil4reps94[, -1], xlab = "Cycle", ylab = "RFU",
    main = "dil4reps94 data set", type = "l", lty = 1, lwd = 2, col = colors
)
mtext("A", cex = 1.5, side = 3, adj = 0, font = 2)

colors <- c("#FF00004D", "#80FF004D", "#00FFFF4D", "#8000FF4D")

matplot(
  data_normalized[, 1], data_normalized[, -1],
  xlab = "Cycle", ylab = "normalized RFU", main = "dil4reps94 data set\nsubset",
  type = "l", lty = 1, lwd = 2, col = colors
)
for (i in 1L:(ncol(data_normalized) - 1)) {
  points(
    res_head2tailratio[[i]]$x_roi, res_head2tailratio[[i]]$y_roi,
    col = colors[i], pch = 19, cex = 1.5
  )
  abline(res_head2tailratio[[i]]$fit, col = colors[i], lwd = 2)
}

mtext("B", cex = 1.5, side = 3, adj = 0, font = 2)
```

### ``hookreg()`` and ``hookregNL()`` - Functions to Detect Hook Effekt-like Curvatures\label{section_hookreg}

``hookreg()`` and ``hookregNL()`` are functions to detect amplification curves 
bearing a hook effect or negative slope at the end of the amplification curve. 
Both functions calculate the slope and intercept of an amplification curve data. 
The idea is that a strong negative slope at the end of an amplification curve is 
indicative for a hook effect.

Amplification curves with a hook effect like curvature are characterized by a 
negative trend in the late phase of the amplification reaction 
(\autoref{plot_hookreg} A, curve F1.1, F1.2, F2.1, F2.2, F3.1 and F3.2).

```{r, eval=TRUE, echo=TRUE}
# Calculate slope and intercept on noise (negative) amplification curve data
# for the last eight cycles.
options(warn = -1)
library(qpcR)
library(magrittr)

res_hook <- sapply(2:ncol(boggy), function(i) {
  hookreg(x = boggy[, 1], y = boggy[, i])
}) %>%
  t() %>%
  data.frame(sample = colnames(boggy)[-1], .)


data_colors <- rainbow(ncol(boggy[, -1]), alpha = 0.5)
cl <- kmeans(na.omit(res_hook[, 2:3]), 2)$cluster
```

The results of the ``hookreg()`` analysis were transferred to a tabular format.

```{r, eval=TRUE, echo=TRUE, results='asis'}
library(xtable)
print(xtable(
  res_hook, caption = "Screening results for the analysis with the 
             hookreg algorithm. Samples withe a value of 1 in the hook column 
             had all a hook effect like curvature. The samples F4.1, F4.2, F5.1, 
             F5.2, F6.1 and F6.2 miss entries because the hoogreg algorithm 
             could not fit a linear model. This is an expected behavior, since 
             these amplification curves did not have a hook effect like 
             curvature.",
  label = "tablehookreg"
), include.rownames = FALSE, comment = FALSE, caption.placement = "top", 
    size = "\\tiny")
```

In \autoref{tablehookreg} is shown that the first amplification curves (F1.1, 
F1.2, F2.1, F2.2, F3.1 and F3.2) appear the have a hook effect-like curvature 
("hook" column=1.00). The function estimate reliably the start of the 
hook effect-like region.

The clusters for amplification curve were determined by k-means clustering in 
this example. Next we plot the results of the analysis (Figure 
\ref{plot_hookreg}). For the visualization the intercepts was plotted against 
the slope with the clusters as determined by k-means clustering.

```{r, echo=TRUE, fig.cap="\\label{plot_hookreg}Analysis of amplification curves for hook effect-like structures. ", fig.height=5, fig.width=7}
par(mfrow = c(1, 2))
matplot(
  x = boggy[, 1], y = boggy[, -1], xlab = "Cycle", ylab = "RFU",
  main = "boggy data set", type = "l", lty = 1, lwd = 2, col = data_colors
)
legend(
  "topleft", as.character(res_hook$sample), pch = 19,
  col = data_colors, bty = "n"
)
mtext("A", cex = 1.5, side = 3, adj = 0, font = 2)

plot(
  res_hook$intercept, res_hook$slope, pch = 19, cex = 2, col = data_colors,
  xlab = "intercept", ylab = "Slope",
  main = "Hook Effect-like Curvature\nboggy data set"
)
mtext("B", cex = 1.5, side = 3, adj = 0, font = 2)
points(res_hook$intercept, res_hook$slope, col = cl, pch = cl, cex = cl)
legend(
  "topright", c("Strong Hook effect", " Weak Hook effect"),
  pch = c(1, 2), col = c(1, 2), bty = "n"
)
text(res_hook$intercept, res_hook$slope, res_hook$sample)
```

Further examples are shown in the manual for the ``hookregNL()`` function.

### ``mblrr()`` - A Function Perform the Quantile-filter Based Local Robust Regression \label{section_mblrr}

``mblrr()`` is a function to perform the Median based Local Robust Regression (m b l r r) from a quantitative PCR experiment. In detail, this function attempts to break the amplification curve in two ROIs (head (~background) and tail (~plateau)). Subsequent, a robust linear regression analysis (``lmrob()``) is preformed individually on both parts. The rationale behind this analysis is that the slope and intercept of an amplification curve differ in the background and plateau region. In the example shown below, the samples "P01.W19", "P06.W35", "P33.W66", "P65.W90", "P71.W23" and "P87.W01" were arbitrarily selected for demonstration purposes \autoref{plot_mblrr}. Another example is shown in \autoref{HCU32}A. Those amplification curves have a slight negative trend in the baseline region and a positive trend in the plateau region (\autoref{section_machine_learning}).

```{r, echo=TRUE, fig.cap="\\label{plot_mblrr}Robust local regression to analyze amplification curves. The amplification curves were arbitrarily selected from the `htPCR` data set. Not the differences in slopes and intercepts (red and green lines). The mblrr() function is presumably useful for data sets which are accompanied by noise and artifacts.m, slop; n, intercept.", fig.height=7, fig.width=7}
options(warn = -1)
library(qpcR)
par(mfrow = c(3, 2))

# Select six amplification curves from the htPCR data set
data <- htPCR[, c(1, 20, 500, 3000, 6000, 6500, 8000)]
for (i in 2:ncol(data)) {
  x <- data[, 1]
  y_tmp <- data[, i] / quantile(data[, i], 0.999)
  res_q25 <- y_tmp < quantile(y_tmp, 0.25)
  res_q75 <- y_tmp > quantile(y_tmp, 0.75)
  res_q25_lm <- try(
    suppressWarnings(lmrob(y_tmp[res_q25] ~ x[res_q25])),
    silent = TRUE
  )
  res_q75_lm <- try(
    suppressWarnings(lmrob(y_tmp[res_q75] ~ x[res_q75])),
    silent = TRUE
  )

  plot(
    x, y_tmp, xlab = "Cycle", ylab = "RFU (normalized)",
    main = colnames(data)[i],
    type = "b", pch = 19
  )
  abline(res_q25_lm, col = "red")
  points(x[res_q25], y_tmp[res_q25], cex = 2.5, col = "red")
  abline(res_q75_lm, col = "green")
  points(x[res_q75], y_tmp[res_q75], cex = 2.5, col = "green")
}
```

Finally, the results of the analysis were printed in a tabular format.

```{r, eval=TRUE, echo=TRUE, results='asis'}
# Load the library xtable for an appealing table output
library(xtable)

# Analyze the data via the mblrr() function

res_mblrr <- do.call(cbind, lapply(2L:ncol(data), function(i) {
  suppressMessages(mblrr(
    x = data[, 1], y = data[, i],
    normalize = TRUE
  )) %>% data.frame()
}))
colnames(res_mblrr) <- colnames(data)[-1]

# Transform the data for a tabular output and assign the results to the object
# output_res_mblrr.

output_res_mblrr <- res_mblrr %>% t()

# The output variable names of the mblrr() function are rather long. For better
# readability the variable names were changed to "nBG" (intercept of head region),
# "mBG" (slope of head region), "rBG" (Pearson correlation of head region),
# "nTP" (intercept of tail region), "mTP" (slope of tail region), "rBG" (Pearson
# correlation of tail region)

colnames(output_res_mblrr) <- c(
  "nBG", "mBG", "rBG",
  "nTP", "mTP", "rTP"
)

print(xtable(
  output_res_mblrr, caption = "mblrr() text intro. nBG, intercept of 
             head region; mBG, slope of head region; rBG, Pearson 
             correlation of head region; nTP, intercept of tail region; mTP, 
             slope of tail region; rBG, Pearson correlation of tail region",
  label = "tablemblrrintroduction"
), comment = FALSE, caption.placement = "top")
```

In another example, the results from the ``mblrr()`` function were combined with the classifications (positive, negative) by a human to apply them in an analysis with Fast and Frugal Trees (FFTrees). A general introduction to decision trees is given in [@quinlan_induction_1986, @luan_signal-detection_2011]. FFTrees belong to class of simple decision rules. In many situations, FFTrees make fast decisions based on a few features (1 - 5). In this example six features were used for the analysis.

The \texttt{FFTrees} package [@FFTrees_package] provides an implementation for the 
\texttt{R} statistical computing language. All that is needed for the present example 
are:

* the data assessed by the ``mblrr()`` function,
* the classification of the amplification curve data by a human,
* and a standard formula, which looks like $outcome \leftarrow var1 + var2 + \ldots$ along with the data arguments.

The function ``FFTrees()`` returns a fast and frugal tree object. This rich object contains the underlying trees and many classification statistics (similar to 
\autoref{section_performeR}).

In the following example, the `testdat` data set from the \texttt{qpcR} package was used. Noise (normal distribution, mean=0, standard deviation=1) was added to the `testdat` data set to show the usefulness and robustness of the ``mblrr()`` function.

```{r, echo=FALSE, fig.cap="\\label{plot_raw_data_FFTrees}Fast and Frugal Trees.", fig.height=5.5, fig.width=11}
# Load the qpcR package to get some data
options(warn = -1)
library(qpcR)

# Use testdat data set, which contains positive and negative amplification
# curves. Note: Noise was added to the raw data.

data <- data.frame(testdat[, 1], testdat[, -1] + rnorm(nrow(testdat[, -1]) * ncol(testdat[, -1]), 0, 0.2))

# Visualize the data
# par(mfrow=c(1,2), las=0, bty="o", oma=c(1,1,1,1))
par(mfrow = c(1, 2), las = 0, bty = "o", oma = c(0, 0, 0, 0))
colors <- rainbow(ncol(data) - 2, alpha = 0.3)
matplot(
  data[, 1], data[, -1], xlab = "Cycle", ylab = "RFU",
  main = "testdat data set", type = "l", lty = 1, lwd = 2, col = colors
)
mtext("A", cex = 1.5, side = 3, adj = 0, font = 2)

# Give classes to the amplification curves (positive=TRUE, negative=FALSE)

human_rater_classification <- c(
  TRUE, TRUE,
  FALSE, FALSE,
  TRUE, TRUE,
  FALSE, FALSE,
  TRUE, TRUE,
  FALSE, FALSE,
  TRUE, TRUE,
  FALSE, FALSE,
  TRUE, TRUE,
  FALSE, FALSE,
  TRUE, TRUE,
  FALSE, FALSE
)

# Plot the data to see if the classification by the human is correct

colors <- as.factor(human_rater_classification)
matplot(
  data[, 1], data[, -1], xlab = "Cycle", ylab = "RFU",
  main = "testdat data set", type = "l", lty = 1, lwd = 2, col = colors
)

mtext("B", cex = 1.5, side = 3, adj = 0, font = 2)
```

```{r, eval=TRUE, echo=TRUE}
# Load the library xtable for an appealing table output
options(warn = -1)
library(FFTrees)

# Analyze the testdat data via the mblrr() function

res_mblrr <- do.call(cbind, lapply(2L:ncol(data), function(i) {
  suppressMessages(mblrr(
    x = data[, 1], y = data[, i],
    normalize = TRUE
  )) %>% data.frame()
}))
colnames(res_mblrr) <- colnames(data)[-1]

# Transform the data for a tabular output and assign the results to the object
# output_res_mblrr.

output_res_mblrr <- res_mblrr %>% t()

# The output variable names of the mblrr() function are rather long. For better
# readability the variable names were changed to "nBG" (intercept of head 
# region), "mBG" (slope of head region), "rBG" (Pearson correlation of head 
# region), "nTP" (intercept of tail region), "mTP" (slope of tail region), 
# "rBG" (Pearson correlation of tail region).

colnames(output_res_mblrr) <- c("nBG", "mBG", "rBG", "nTP", "mTP", "rTP")

output_res_mblrr <- data.frame(
  class = human_rater_classification,
  output_res_mblrr
)

output_res_mblrr.fft <- suppressMessages(FFTrees(
  formula = class ~.,
  data = output_res_mblrr[, c(1, 2, 3, 5, 6)]
))

write.csv(output_res_mblrr, "output_res_mblrr.csv", row.names = FALSE)
``` 

The \autoref{tablemblrr} output.

```{r, data=output_res_mblrr, eval=TRUE, echo=TRUE, results='asis'}
library(xtable)
print(xtable(
  output_res_mblrr, caption = "mblrr() text new. nBG, intercept of 
             head region; mBG, slope of head region; rBG, Pearson 
             correlation of head region; nTP, intercept of tail region; mTP, 
             slope of tail region; rBG, Pearson correlation of tail region",
  label = "tablemblrr"
), include.rownames = FALSE, comment = FALSE, caption.placement = "top")
```

\autoref{plot_FFTrees} shows the Fast and Frugal Trees by using the features nBG (intercept of head region), mBG (slope of head region), rBG (Pearson correlation of head region), nTP (intercept of tail region), mTP (slope of tail region), and rBG (Pearson correlation of tail region).


```{r, echo=FALSE, fig.cap="\\label{plot_FFTrees}Fast and Frugal Trees. nBG, intercept of head region; mBG, slope of head region; rBG, Pearson correlation of head region; nTP, intercept of tail region; mTP, slope of tail region; rBG, Pearson correlation of tail region.", fig.height=11, fig.width=11}
plot(output_res_mblrr.fft, decision.lables = c("Positive", "Negative"))
```

### ``pcrfit_single()`` - A Function to Calculate Features from an Amplification Curve \label{section_pcrfit_single_pcrfit_parallel}

The ``pcrfit_single()`` function is a wrapper for different functions to calculate features from an amplification curve. This includes methods to detect change points or to calculate slopes in specific regions of interest (ROI) of the amplification curve. The following overview gives some comments on the internal algorithms used by the ``pcrfit_single()`` function.

> Note: The ``pcrfit_single()`` function and the ``encu()`` function perform two important pre-processing steps before each calculation. That includes checking whether an amplification curve contains missing values. They are usually automatically imputed. These two functions will only terminate with an error message in extreme cases. In the next step, all values of an amplification curve are normalized to the 99.9% quantile. The normalization is necessary because the amplitude measurement values of amplification curves can be very different. This means that there are considerable differences in the maximum values. Therefore, users of the \texttt{PCRedux} package are advised to take a look at the data sets of amplification curves before starting complex analyses. In order to compare amplification curves from different thermal cyclers, the values should always be scaled systematically using the same method. Although there are other normalization methods (e. g. *minmum-maximum normalization*, see @roediger2015chippcr), the normalization by means of the 99.9% quantile should ensure that the information about the height of the background signal is not lost. That would be the case with a *Min-Max-standardization*. A normalization to the maximum is not recommended, because outliers could have an unintentional influence on the normalization. Consequently, the 99.9% quantile is a pragmatic compromise to take the aforementioned aspects into account in the processing chain.

* Change point analysis
    - Change point analysis (CPA) encompasses methods to identify or estimate single or multiple locations of changes in a series of data points indexed in time order. A change herein refers to a statistical property. There exist several change point algorithms such as the binary segmentation algorithm [@scott_cluster_1974]. CPA is used for example in climatology, econometrics and bioinformatics. [@Killick_2014]
    - ``bcp()`` [\texttt{bcp}]: Performs a change point analysis based on a Bayesian approach.
    - ``e.agglo()`` [\texttt{ecp}]: Performs a multiple change point analysis based on agglomerative hierarchical estimation.

In this study CPA was hypothesized that the number of change points differs between positive (sigmoidal) and negative (noise) amplification curves. For change point analysis the packages \texttt{bcp} [@erdman_bcp:_2007] and \texttt{ecp} [@james_ecp:_2013] were used. From the \texttt{ecp} package the function ``e.agglo()`` and the method "BinSeg" (according to [@scott_cluster_1974]) was used \autoref{plot_bcp}. 

```{r, echo=FALSE, fig.cap="\\label{plot_bcp}Test.", fig.height=6}
# Analyze amplification curves from the testdat data set for changepoints
options(warn = -1)

library(bcp)
library(qpcR)

x_range <- 1L:35

amp_data <- testdat[x_range, c(2, 4)]

# Bayesian analysis of change points
# Positive amplification curve
res_bcp_pos <- bcp(amp_data[, 1])

y2_range <- range(res_bcp_pos$posterior.prob, na.rm = TRUE)

# Negative amplification curve
res_bcp_neg <- bcp(amp_data[, 2])

par(mfrow = c(1, 2))
plot(res_bcp_pos$data, axes = FALSE, xlab = "Cycle", ylab = "RFU", type = "b", lwd = 2, pch = 19)
axis(1, xlim = c(0, 1))
axis(2, xlim = c(0, 1))
mtext("Prob", side = 4, line = 4)
box()

par(new = TRUE)
plot(res_bcp_pos$posterior.prob, axes = FALSE, xlab = "Cycle", ylab = "RFU", type = "b", lwd = 2, pch = 19)
axis(4, ylim = c(0, 1), col = "black", las = 1)
mtext("A    Positive", cex = 1.5, side = 3, adj = 0, font = 2)

# -----

plot(res_bcp_neg$data, axes = FALSE, xlab = "Cycle", ylab = "RFU", type = "b", lwd = 2, pch = 19)
axis(1, xlim = c(0, 1))
axis(2, ylim = c(0, 1))
mtext("Prob", side = 4, line = 4)
box()


par(new = TRUE)
plot(res_bcp_neg$posterior.prob, axes = FALSE, xlab = "Cycle", ylab = "RFU", type = "b", lwd = 2, pch = 19)
axis(4, ylim = c(0, 1), col = "black", las = 1)
mtext("B    Negative", cex = 1.5, side = 3, adj = 0, font = 2)
```

* Test of an amplification reaction
    - ``amptester()`` [\texttt{chipPCR}]: Tests with different statistical methods, if the amplification is positive of negative.
    - ``bg.max()`` [\texttt{chipPCR}]: Calculates estimates for the start and end of the background region and the end of the exponential amplification reaction. The estimates of the ``bg.max`` function are normalized to the total cycle number.
    - ``takeoff()`` [\texttt{qpcR}]: Calculates the qPCR takeoff point using functionality.

* Derivatives
    - ``diffQ()`` [\texttt{MBmca}]: 
    - ``diffQ2()`` [\texttt{MBmca}]: 
    - ``mcaPeaks()`` [\texttt{MBmca}]:
    
    An example is given for the internal parameter ``diffQ2_slope`` which is calculated from the slope determined by a linear model of the data points from the minimum and maximum of the second derivative. The coordinates of the minimum and the maximum were determined as described in @roediger_RJ_2013. In the following example, the data 
    
```{r, echo=TRUE, fig.cap="\\label{diffQ2_slope}Analysis of the amplification curve via diffQ2()."}
# Load example data (sample F6.1) from the testdat data set
options(warn = -1)
library(qpcR)
library(magrittr)
# Load MBmca package to calculate the minimum, and the maximum of the second
# derivative

library(MBmca)

par(mfrow = c(2, 2))

diffQ2_slope_fun <- function(data, cyc=1, rfu=2) {
  # Calculate the minimum, and the maximum of the second
  # derivative and assign it to the object res_diffQ2

  sample <- colnames(data)[rfu]

  data_tmp <- data.frame(cyc = data[, cyc], rfu = data[, rfu])

  res_diffQ2 <- suppressMessages(diffQ2(data_tmp, plot = FALSE, fct = min))

  # Build a linear model from der second derivative of res_diffQ2

  res_diffQ2_lm <- lm(res_diffQ2[["yTm1.2.D2"]] ~ res_diffQ2[["xTm1.2.D2"]])

  data_tmp %>% plot(
    ., xlab = "Cycle", ylab = "RFU", main = "", type = "l",
    lty = 1, lwd = 2, col = "black"
  )
  abline(v = res_diffQ2[["xTm1.2.D2"]], col = "grey", lwd = 2)
  mtext(paste("A", sample), side = 3, adj = 0, font = 1)

  plot(
    res_diffQ2[["xTm1.2.D2"]], res_diffQ2[["yTm1.2.D2"]], pch = 19, cex = 1.5,
    xlab = "Cycle", ylab = "dd(RFU)/d(Cycle)",
    main = ""
  )
  abline(res_diffQ2_lm, col = "blue", lwd = 2)
  legend("topleft", paste("Slope: ", signif(
    coefficients(res_diffQ2_lm)[2],
    4
  )), bty = "n")
  mtext("", side = 3, adj = 0, font = 2)
}

for (i in c(2, 4)) {
  diffQ2_slope_fun(testdat, cyc = 1, rfu = i)
}
```

* Areas, Ratios and Slopes
    - ``head2tailratio()`` [\texttt{PCRedux}]: Calculates an estimate for the head to tail fluorescence, normalized to the slope. See \autoref{section_head2tailratio}.
    - ``earlyreg()`` [\texttt{PCRedux}]: Calculates the slope and intercept of an amplification curve data from a quantitative PCR experiment. See \autoref{section_earlyreg}.
    - ``mblrr()`` [\texttt{PCRedux}]: See \autoref{section_mblrr}.
    - ``polyarea()`` [\texttt{pracma}]: Calculates the area of a polygon defined by the vertices with coordinates of the cycles and fluorescence, based on the Gauss polygon area formula.
    - ``efficiency()`` [\texttt{qpcR}]: Determines the Cq (first derivative maximum, second derivative maximum) and other parameters. 
    - ``LRE()`` [\texttt{qpcR}]: Calculates the qPCR efficiency by the 'linear regression of efficiency' method using functionality from the \texttt{qpcR} package.
    - ``sliwin()`` [\texttt{qpcR}]: Calculates the qPCR efficiency by the 'window-of-linearity' method using functionality from the \texttt{qpcR} package. 
    
* Hook-effect
    - ``hookreg() & hookregNL()`` [\texttt{PCRedux}]:

* Autocorrelation
    - ``autocorrelation_test()`` [\texttt{PCRedux}]: Test for an autocorrelation of amplification curve data from a quantitative PCR experiment. See \autoref{section_autocorrelation_test}. 



``pcrfit_single()`` is a wrapper function, which calculates more than 40 potential features from an amplification curve. This comes at a cost, since several internal functions are computational very intensive [@porzelius_easier_2009, @schmidberger_2009]. For example, ``pcrfit()`` function (\texttt{qpcR} package) in ``pcrfit_single()`` fits and optimizes eight non-linear (sigmoid) models to the amplification curve data.
 ``encuf()`` (ENcode CUrves) is a relative of the ``pcrfit_single()`` function.  Similarly, this function calculates numerous but with an emphasis on features extraction of large amplification curve data sets. The ``pcrfit_single()`` function is performing the analysis for a single process and the ``pblapply()`` function from the \texttt{pbapply} package is used internally to is delivers a progress bar and leverages parallel processing. Examples are given in the documentation of the ``encu()`` function.

Parallel computing technologies saves scientists time in their routine tasks of analyzing experimental data. Information about parallel computing technologies in \texttt{R} are available from @eddelbuettel_cran_2017. 

```{r, eval=TRUE, echo=FALSE}
data(testdat, package = "qpcR")
x <- testdat[, 1]
y.pos <- testdat[, 2]
y.neg <- testdat[, 4]
nh <- trunc(length(x) * 0.20)
nt <- trunc(length(x) * 0.15)

y.pos.head <- head(y.pos, n = nh)
y.neg.head <- head(y.neg, n = nh)
y.pos.tail <- tail(y.pos, n = nt)
y.neg.tail <- tail(y.neg, n = nt)

lb.pos <- median(y.pos.head) + 2 * mad(y.pos.head)
ub.pos <- median(y.pos.tail) - 2 * mad(y.pos.tail)

lb.neg <- median(y.neg.head) + 2 * mad(y.neg.head)
ub.neg <- median(y.neg.tail) - 2 * mad(y.neg.tail)

res.shapiro.pos <- shapiro.test(y.pos)
res.shapiro.neg <- shapiro.test(y.neg)

res.wt.pos <- wilcox.test(head(y.pos, n = nh), tail(y.pos, n = nt), alternative = "less")
res.wt.neg <- wilcox.test(head(y.neg, n = nh), tail(y.neg, n = nt), alternative = "less")

###
RGt <- function(y) {
  ws <- ceiling((15 * length(y)) / 100)
  if (ws < 5) {
    ws <- 5
  }
  if (ws > 15) {
    ws <- 15
  }
  y.tmp <- na.omit(y[-c(1:5)])
  x <- 1:length(y.tmp)
  suppressWarnings(
    res.reg <- sapply(1L:(length(y.tmp)), function(i) {
      round(summary(lm(y.tmp[i:c(i + ws)] ~ x[i:c(i + ws)]))[["r.squared"]], 4)
    })
  )

  # Binarize R^2 values. Everything larger than 0.8 is positive
  res.LRt <- res.reg
  # Define the limits for the R^2 test
  res.LRt[res.LRt < 0.8] <- 0
  res.LRt[res.LRt >= 0.8] <- 1
  # Seek for a sequence of at least six positive values (R^2 >= 0.8)
  # The first five measuring point of the amplification curve are skipped
  # because most technologies and probe technologies tend to overshot
  # in the start (background) region.
  res.out <- sapply(5L:(length(res.LRt) - 6), function(i) {
    ifelse(sum(res.LRt[i:(i + 4)]) == 5, TRUE, FALSE)
  })
  out <- cbind(1L:(length(y.tmp)), res.reg)
  # res.out
}
```

```{r, fig.cap="Figure caption Positive. Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\\label{statistical_methods_positive}"}

layout(matrix(c(1, 1, 2, 2, 1, 1, 3, 3), 2, 4, byrow = TRUE), respect = TRUE)

plot(x, y.pos, xlim = c(1, 50), ylim = c(-0.1, 15.5), xlab = "Cycle", 
     ylab = "RFU", main = "", type = "b", pch = 19)
mtext("A    Positive amplification", side = 3, adj = 0, font = 2)
abline(v = c(nh, length(x) - nt), lty = 3)

abline(h = lb.pos, lty = 2, col = "red")
text(35, 1.5, "Noise\nmedian + 2 * MAD", col = "red", cex = 1)

abline(h = ub.pos, lty = 2, col = "green")
text(35, 10, "Signal\nmedian - 2 * MAD", col = "green", cex = 1)

arrows(4.5, 12.5, 42.5, 12.5, length = 0.1, angle = 90, code = 3)
text(25, 14.5, paste("W=", signif(res.wt.pos$statistic, 3), "\np-value=", signif(res.wt.pos$p.value, 3)))
text(25, 5, paste("Fold change: \n", round(ub.pos / lb.pos, 2)))

qqnorm(y.pos, pch = 19, main = "")
legend("bottomright", paste("W=", signif(res.shapiro.pos$statistic, 3), 
                            "\np-value=", signif(res.shapiro.pos$p.value, 3)), bty = "n")
mtext("B", side = 3, adj = 0, font = 2)
qqline(y.pos, col = "orange", lwd = 2)

plot(RGt(y.pos), xlab = "Cycle", ylab = expression(R ^ 2), main = "", pch = 19, 
     type = "b")
mtext("C    LRt", side = 3, adj = 0, font = 2)
abline(h = 0.8, col = "black", lty = 2)
```

```{r, fig.cap="Figure caption Negative Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\\label{statistical_methods_negative}"}
layout(matrix(c(1, 1, 2, 2, 1, 1, 3, 3), 2, 4, byrow = TRUE), respect = TRUE)

plot(x, y.neg, xlim = c(1, 50), ylim = c(-0.03, 0.07), xlab = "Cycle", ylab = "RFU", main = "", type = "b", pch = 19)
mtext("B    Negative amplification", side = 3, adj = 0, font = 2)
abline(v = c(nh, length(x) - nt), lty = 3)

abline(h = lb.neg, lty = 2, col = "red")
text(10, 0.04, "Noise\nmedian + 2 * MAD", col = "red", cex = 1)

abline(h = ub.neg, lty = 2, col = "green")
text(40, -0.02, "Signal\nmedian - 2 * MAD", col = "green", cex = 1)

arrows(4.5, 0.04, 42.5, 0.04, length = 0.1, angle = 90, code = 3)
text(25, 0.06, paste("W=", res.wt.neg$statistic, "\np-value=", signif(res.wt.neg$p.value, 3)))
legend("topright", paste("Fold change: \n", round(ub.neg / lb.neg, 2)), bty = "n")


qqnorm(y.neg, pch = 19, main = "")
legend("bottomright", paste("W=", signif(res.shapiro.neg$statistic, 3), "\np-value=", signif(res.shapiro.neg$p.value, 3)), bty = "n")
mtext("B", side = 3, adj = 0, font = 2)
qqline(y.neg, col = "orange", lwd = 2)

plot(RGt(y.neg), xlab = "Cycle", ylab = expression(R ^ 2), main = "", pch = 19, type = "b")
mtext("C    LRt", side = 3, adj = 0, font = 2)
abline(h = 0.8, col = "black", lty = 2)
```


### Parallel programming

To process high data volumes and to deal with speed issues several \texttt{R} packages for parallelization were evaluated [@vera_r_parallel_2008, @porzelius_easier_2009, @boehringer_dynamic_2013]. For the calculation of the curve parameters the custom-made function *pcrfit_parallel* was developed (see \autoref{pcrfit_parallel}). In particular, to benchmark and evaluate the performance of multiple learners on multiple tasks studies quickly become resource-demanding. Therefore, packages such as **mlr** support natively parallelization [@bischl_mlr:_2010].

The code block below shows an example for a parallelized version of ``pcrfit_single()``. This function is called ``pcrfit_parallel()``, which is intended for users who wish to calculate the features of a large amplification 
curve data set. This function appears to be works on Linux systems. On Windows 
systems error messages were reported. ``pcrfit_parallel()`` makes use of 
parallelized code to make use of multi-core architectures. In this function we 
import from the \texttt{parallel} package the ``detectCores()`` function. This function determines the number of available cores. Function from the \texttt{foreach} package (e.g., ``%dopar%``, ``foreach()``) are used for the further organization of the CPU usage. The ``pcrfit_single()`` performs the analysis for a single process.

* The parameter `data` is the data set containing the cycles and fluorescence amplitudes.
* The parameter `n_cores` defines the numbers of cores that should be left unused by this function. 

By default, ``pcrfit_parallel()`` is using only one core (`n_cores=1`). 
`n_cores="all"` uses all available cores. The output of the 
``pcrfit_parallel()`` function is similar to the ``pcrfit_single()`` function.


```{r, echo=TRUE, eval=FALSE}
# Copy and paste the code to an R console to evaluate it

library(parallel)
library(doParallel)

pcrfit_parallel <- function(data, n_cores=1) {
  # Determine the number of available cores and register them
  if (n_cores == "all") {
    n_cores <- detectCores()
  }

  registerDoParallel(n_cores)

  # Prepare the data for further processing
  # Normalize RFU values to the alpha percentile (0.999)
  cycles <- data.frame(cycles = data[, 1])
  data_RFU <- data.frame(data[, -1])
  data_RFU_colnames <- colnames(data_RFU)
  data_RFU <- sapply(1L:ncol(data_RFU), function(i) {
    data_RFU[, i] / quantile(data_RFU[, i], 0.999, na.rm = TRUE)
  })
  colnames(data_RFU) <- data_RFU_colnames

  # just to shut RCHeck for NSE we define ith_cycle
  ith_cycle <- 1

  run_res <- foreach::foreach(
    ith_cycle = 1L:ncol(data_RFU),
    .packages = c(
      "bcp", "changepoint", "chipPCR", "ecp", "MBmca",
      "PCRedux", "pracma", "qpcR", "robustbase",
      "zoo"
    ),
    .combine = rbind
  ) %dopar% {
    suppressMessages(pcrfit_single(data_RFU[, ith_cycle]))
  }


  res <- cbind(runs = colnames(data_RFU), run_res)

  rownames(res) <- NULL

  res
}

# Calculate curve features of an amplification curve data. Note: Not all
# CPU cores are used. If need set "all" to use all available cores.
# In this example the testdat data set from the qpcR package is used.
# The samples F1.1 and F1.2 are positive amplification curves. The samples
# F1.3 and F1.4 are negative.
options(warn = -1)
library(qpcR)
res_pcrfit_parallel <- pcrfit_parallel(testdat[, 1:5])
res_pcrfit_parallel
```

### ``performeR()`` - Performance Analysis for Binary Classification \label{section_performeR}

Statistical modeling and machine learning can be powerful but expose a risk to the user by introducing an unexpected bias. This may lead to an overestimation of the performance. The assessment of the performance by the sensitivity and specificity is fundamental to characterize the performance of a classifier or screening test [@james_introduction_2013].

> Sensitivity is the percentage of true decisions that are identified and specificity is the percentage of negative decision that are correctly identified.

An example for the application of the ``performeR()`` function is shown in \autoref{section_autocorrelation_test}.


Abbreviations: TP, true positive; FP, false positive; TN, true negative; FN, false negative

Measure                                    | Formula
-------------------------------------------|-----------------------------------------------
Sensitivity - TPR, true positive rate      | $TPR=\frac{TP}{TP + FN}$
Specificity - SPC, true negative rate      | $SPC=\frac{TN}{TN + FP}$
Precision - PPV, positive predictive value | $PPV=\frac{TP}{TP +  FP}$
Negative predictive value - NPV            | $NPV=\frac{TN}{TN + FN}$
Fall-out, FPR, false positive rate         | $FPR=\frac{FP}{FP + TN}=1 - SPC$
False negative rate - FNR                  | $FNR=\frac{FN}{TN + FN}=1 - TPR$
False discovery rate - FDR                 | $FDR=\frac{FP}{TP + FP}=1 - PPV$
Accuracy - ACC                             | $ACC= \frac{(TP + TN)}{(TP + FP + FN + TN)}$
F1 score - F1                              | $F1=\frac{2TP}{(2TP + FP + FN)}$
Matthews correlation coefficient - MCC     | $MCC=\frac{(TP*TN - FP*FN)}{\sqrt{(TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)}}$
Likelihood ratio positive - LRp            | $LRp=\frac{TPR}{1-SPC}$
Cohen''s kappa (binary classification)     | $\kappa=\frac{p_{0}-p_{c}}{1-p_{0}}$


### ``qPCR2fdata()`` - A Helper Function to Convert Amplification Curve Data to the `fdata` Format \label{section_qPCR2fdata}

``qPCR2fdata()`` is a helper function to convert qPCR data to the functional `fdata` class as published by @Febrero_Bande_2012. This function prepares the data for further analysis, which includes utilities for functional data analysis. For example, it this can be used to determine the similarity measures between amplification curves shapes by the Hausdorff distance. Similarity herein refers to the difference in spatial location of two \textit{objects} (e.g., amplification curves). Objects with a close distance are presumably more similar. For single objects (e.g., points) one can use a vector distance, such as the Euclidean distance. The Hausdorff distance is an approximation of a shape metrics to define similarity measures between shapes. [@charpiat_shape_2003]. Several variants of the Hausdorff distance have been described (e.g., Minimal Hausdorff distance, Average Hausdorff distance, \textit{k}-th ranked Hausdorff distance) [@herrera_multiple_2016].

The ``qPCR2fdata()`` function takes a data set containing the amplification cycles (first column) and the fluorescence amplitudes (subsequent columns) as input. Noise and missing values may affect the analysis adversely. Therefore, an instance of the ``CPP()`` function (\texttt{chipPCR} package [@roediger2015chippcr]) was integrated in ``qPCR2fdata()``. If \textit{preprocess=TRUE} in ``qPCR2fdata()``, then all curves are smoothed (Savitzky-Golay smoother), missing values are imputated and outliers in the initial phase get removed. The non-smoothed amplification curves (\autoref{qPCR2fdata}A) have slightly more noise than the smoothed amplification curves (\autoref{qPCR2fdata}C).

The following example illustrates the usage for the `testdat` data set.

```{r echo=TRUE, message=FALSE, warning=FALSE}
# Calculate the Hausdorff distance of the amplification curves
# cluster the curves.
# Load additional packages for data and pipes.
options(warn = -1)
library(qpcR)
library(chipPCR)
library(fda.usc)
library(magrittr)

# Convert the qPCR data set to the fdata format
# Use unprocessed data from the testdat data set
res_fdata <- qPCR2fdata(testdat)

# Use preprocessed data (preprocess=TRUE) from the testdat data set
res_fdata_preprocessed <- qPCR2fdata(testdat, preprocess = TRUE)

# Extract column names and create rainbow color to label the data
res_fdata_colnames <- testdat[-1] %>% colnames()
data_colors <- rainbow(length(res_fdata_colnames), alpha = 0.5)
```

Hierarchical cluster analysis is widely applied in data analysis. This method uses the elements of a proximity matrix to generate a dendrogram. The dendrogram can can be used to further analyze the clusters. Although there are methods to determine the number of clusters \textit{k} in the present workflow the number of clusters was determined visually [@cook_interactive_2007]. The results of the cluster analysis led the identical clusters for the non-smoothed (\autoref{qPCR2fdata}B) and the smoothed (\autoref{qPCR2fdata}D) amplification curves. Two large clusters are visible. A deeper inspection shows that the samples are correctly assigned to a cluster of positive or negative amplification curves. Moreover, the later increase of the fluorescence is reflected in the positive cluster.

```{r, echo=TRUE, fig.cap="\\label{qPCR2fdata} Grouping of amplification curves of the `testdat` data set via Hausdorff distance. A) The amplification curves were converted with the qPCR2fdata() function and subsequent processed by a cluster analysis using the Hausdorff distance. C) The \\textbf{smoothed} amplification curves show fewer noised than the non-smoothed data. They were also with the qPCR2fdata() function and subsequent processed by a cluster analysis based on the Hausdorff distance.", fig.height=11, fig.width=11}
# Plot the converted qPCR data
par(mfrow = c(2, 2))
res_fdata %>% plot(
  ., xlab = "Cycle", ylab = "RFU", main = "testdat", type = "l",
  lty = 1, lwd = 2, col = data_colors
)
legend(
  "topleft", as.character(res_fdata_colnames), pch = 19,
  col = data_colors, bty = "n", ncol = 2
)
mtext("A", cex = 1.5, side = 3, adj = 0, font = 2)
# Calculate the Hausdorff distance (fda.usc) package and plot the distances
# as clustered data.

res_fdata_hclust <- metric.hausdorff(res_fdata)
res_hclust <- hclust(as.dist(res_fdata_hclust))

plot(res_hclust, main = "Clusters of the amplification\n
curves as calculated by the Hausdorff distance")
mtext("B", cex = 1.5, side = 3, adj = 0, font = 2)
rect(0.5, -3, 12.25, 0.5, border = "red")
text(7, 1, "negative", col = "red")
rect(12.5, -3, 24.5, 0.5, border = "green")
text(14, 1, "positive", col = "green")

# Repeat the plot and the cluster analysis for the preprocessed data
res_fdata_preprocessed %>% plot(
  ., xlab = "Cycle", ylab = "RFU", main = "testdat", type = "l",
  lty = 1, lwd = 2, col = data_colors
)
legend(
  "topleft", as.character(res_fdata_colnames), pch = 19,
  col = data_colors, bty = "n", ncol = 2
)
mtext("C", cex = 1.5, side = 3, adj = 0, font = 2)
# Calculate the Hausdorff distance (fda.usc) package and plot the distances
# as clustered data from preprocessed amplification curves.

res_fdata_hclust_preprocessed <- metric.hausdorff(res_fdata_preprocessed)

res_hclust_preprocessed <- hclust(as.dist(res_fdata_hclust_preprocessed))
plot(res_hclust_preprocessed, main = "Clusters of the preprocessed 
    amplification\n curves as calculated by the Hausdorff distance")
rect(0.5, -3, 12.25, 0.5, border = "red")
text(7, 1, "negative", col = "red")
rect(12.5, -3, 24.5, 0.5, border = "green")
text(14, 1, "positive", col = "green")
mtext("D", cex = 1.5, side = 3, adj = 0, font = 2)
```

As final step followed an analysis of the clusters. Since the distance based on the Hausdorff metric was already done the next steps involved the ``cutree()`` function from the \texttt{stats} package to split the dendrogram into smaller junks. A priori was defined that two classes (\textit{positive} & \textit{negative}) are expected. Therefore, the \textit{group} parameter was set to \textit{k}=2 in the ``cutree()``. The result is shown in \autoref{tablecutreehausdorff}.

```{r echo=TRUE, message=FALSE, warning=FALSE, results='asis'}
# Cluster of the unprocessed amplification curves
res_cutree <- cutree(res_hclust, k = 2)

# Cluster of the pre-processed amplification curves
res_cutreepreprocessed <- cutree(res_hclust_preprocessed, k = 2)

cutree_results <- data.frame(
  names(res_cutree), unprocessed = res_cutree,
  preprocessed = res_cutreepreprocessed
)

colnames(cutree_results) <- c("Sample", "Unprocessed", "Pre-processed")

library(xtable)
print(
  xtable(
    cutree_results, caption = "Clusters of the amplification curves after an 
    analysis using the Hausdorff distance. The amplification curves of the 
    `testdat` data set remained as raw data or were pre-processed (smoothed). 
  Subsequent, the amplification curves were converted by the ``qPCR2fdata()``. 
  The converted data were subjected to a cluster analysis (Hausdorff distance). 
  All sample were correctly assigned to cluster 1 (positive) or cluster 2 
  (negative).",
    label = "tablecutreehausdorff"
  ),
  include.rownames = FALSE, caption.placement = "top", comment = FALSE
)
```

This analysis workflow can be used to cluster amplification curve data according to the shape. It is worth to mention that the calculation of the distances is a computing expensive steps that can take a lot of computing time dependent on the number of amplification curves. This workflow is useful to form smaller groups of similar amplification curves. This step can be used to form smaller groups of amplification curves for classification tasks. Other distance metric than the Hausdorff distance should also be considered. For example, @luo_learning_2010 showed how for image data how shape models using local curve segments with multiple types of distance metrics can improve the shape classification and detection results.


### ``visdat_pcrfit()`` - A Function to Visualize the Content of Data From an Analysis with the ``pcrfit_single()`` Function \label{section_visdat_pcrfit_parallel}

The ``visdat_pcrfit()`` function makes use of the ``vis_dat`` function from the \texttt{visdat} package by @Tierney2017. The samples "A01", "A06" and "A10" from the ``C126EG685`` of the \texttt{chipPCR} package were analyzed.

```{r, eval=TRUE, echo=TRUE}
# Calculate curve features of an amplification curve data set.
# Use the C126EG685 data set from the chipPCR package and analyze the samples
# A01, A02, A04 and B05.

library(chipPCR)

res_1 <- cbind(runs = "A01", pcrfit_single(C126EG685[, 2]))
res_2 <- cbind(runs = "A02", pcrfit_single(C126EG685[, 3]))
res_3 <- cbind(runs = "A04", pcrfit_single(C126EG685[, 5]))
res_4 <- cbind(runs = "B04", pcrfit_single(C126EG685[, 17]))


res <- rbind(A01 = res_1, A02 = res_2, A04 = res_3, B04 = res_4)
```

Finally, the data were visualized with the ``visdat_pcrfit()`` function. In this example the static plot is shown (\autoref{visdat_pcrfit_plot}). It is also possible to run the function interactively by setting the parameter "interactive=TRUE". In this case starts an interactive, browser-based charting library that uses ECMA Script. The interactive plot are rendered entirely locally, through a HTML widgets framework

```{r, echo=TRUE, fig.cap="\\label{visdat_pcrfit_plot}Application of visdat_pcrfit() for the visualization of the data structure after an analysis by pcrfit_single().", fig.height=11, fig.width=11}
# Show all results in a plot. Not that the interactive parameter is set to
# FALSE.

visdat_pcrfit(res, type = "all", interactive = FALSE)
```

## Examples for Machine Learning \label{section_machine_learning}

### Brief Introduction to Machine Learning \label{MachineLearingPCR}

Data mining algorithms and machine learning can be used for descriptive and predictive tasks in the analysis of complex data sets.  Data mining uses specific methods (statistical interference, software engineering) and domain knowledge to obtain a better understanding of the data and to extract "hidden knowledge" from the preprocessed data [@herrera_multiple_2016]. A part of the data mining process is the description of the data, the exploration of the data and the search for connections and causes. All this implies that a human being interacts with the data at different stages of the process. The human being is therefore always a part of the workflow in data mining. 
Machine learning, on the other hand, uses instructions in computer modules to create models that can be used to make predictions. In machine learning, the human being is much less necessary in the entire process. In the ideal case, this should achieve a high degree of objectivity. It is not always possible to justify this ideal, because the algorithms as well as the human instance bring bias. 

Well-known examples of machine learning technologies are Decision Trees (DT), Boosting, Random Forests (RF), Support Vector Machines (SVM), Na$\"{\i}$ve Bayes, generalized linear models (GLM), logistic regression and deep neural networks (DNN)[@lee_statistical_2010].  Recently, the topic of"Reinforcement Learning" has become more and more the focus of interest. Decision trees are a classic approach to machine learning[@quinlan_induction_1986]. Here relatively simple algorithms and simple tree structures are used to create a model. \texttt{R} offers several packages like \texttt{party}[@hothorn_unbiased_2006] and \texttt{rpart}[@rpart_2017] for creating decision trees. Graphical user interface like \texttt{Rattle} @williams_rattle:_2009] offer such tools for data mining in \texttt{R}.

Note that technologies such as logistic regression are used in both data science and machine learning. Logistical regression is used in data science to gain knowledge about the relationship. It can be used e. g. for the analysis of amplification curve data from quantitative real-time PCRs to describe the relationship between the number of cycles and the emitted fluorescence (see \autoref{section_DataAnalysis}). For this purpose, a logistical function must be fitted to the data set. With the model obtained, predictions can be made. For example, the position of the maximum of the second derivation can be calculated from this.

In Machine Learning, variables are features that are used to train a model [@saeys_review_2007]. The model should then be able to bring new unknown data into a meaningful context. The selected features have a significant influence on the accuracy of the model. Therefore, it is important to define a number of useful features, to test them intensively or to generate new features. With regard to amplification curves, only a few feautres have been described in the literature so far. The range of features that can be created with the algorithms of the \texttt{PCRedux} package were probably the most extensive collection at the time of first release (summer 2017). Previously, only a few characteristics of amplification curves were described in the literature. Thus, it would be too few to use them extensively for machine learning with qPCR data. An application of those for machine learning could also not be found.

During machine learning, processes (algorithms) are used to create models with tunable parameters. These models automatically adapt their performance to the information (features) from the data. The three following classes of principles of machine learning are described in the literature:

* *Supervised learning*: These algorithms (e. g. logistic regression, SVM, DT, RF) learn from a training data set of labelled and annotated data (e. g."positive" and "negative"). It is used for a generalized model of all data. Unlike unsupervised learning, these algorithms use error or reward signals to evaluate the quality of a solution found. [@bischl_mlr:_2010, @greene_big_2014, @igual_introduction_2017]
* *Unsupervised learning*: Algorithms, such as k-means clustering, kernel density estimation, LDA or PCA learn from training data sets of unlabeled or non-annotated data to find hidden structures according to geometric or statistical criteria. [@bischl_mlr:_2010, @greene_big_2014, @igual_introduction_2017]
* *Reinforcement Learning*: The algorithms learn by reinforcement from criticism. The criticisms inform the algorithm about the quality of the solution found. But the criticism says nothing about how to improve. These algorithms iteratively search the improved solution in the entire solution space. [@bischl_mlr:_2010, @igual_introduction_2017]


The algorithms of machine learning consist of several steps including careful data pre-processing and quality management. In a first step, relatively large datasets of known characteristic vectors have to be collected, measured and calculated as raw data. In a second step, these characteristics are used to classify unknown feature vectors using the Machine Learning algorithm.

The following chapter focuses on the exemplary application of feature vectors from amplification curves, which can be used for automatic classification by machine learning. 

The first task of this work was to develop and define potential characteristics. This has been described in chapter \autoref{section_Functions_of_PCRedux}. These features can be extracted from virtually all amplification curve data. The pre-processing of data is described in detail in chapter \autoref{section_data_sets}.

The ideal machine learning algorithm for the analysis of amplification curve data was not known beforehand. For this reason, it was necessary to speculate which characteristics should be extracted by the processing algorithm and broken down into characteristic vectors.


The amplification curves had to be divided into training data and test data from the entire data set, which were manually categorized, at random.

The following example illustrates the usage for the `HCU32_aggR.csv` data set 
from the 32 channel VideoScan heating and cooling unit. In this experiment the
bacterial gene *aggR* from *E. coli* was amplified in 32 replicate qPCR reactions. Details of the experiment are described in the manual of the \texttt{PCRedux} package.
The ambition was to test if the 32 amplification curves of the qPCR reaction 
are identical. As before, the data were processed with the ``qPCR2fdata()`` 
function and compared by the the Hausdorff distance. Ideally, the amplification 
curves form only few clusters.

```{r echo=TRUE, message=FALSE, warning=FALSE}
# Calculate slope and intercept on positive amplification curve data from the
# VideoScan 32 cavity real-time PCR device.
# Load additional packages for data and pipes.
options(warn = -1)
library(data.table)
library(fda.usc)
library(magrittr)

# Load the qPCR data from the HCU32_aggR.csv data set
# Convert the qPCR data set to the fdata format

filename <- system.file("HCU32_aggR.csv", package = "PCRedux")
data_32HCU <- fread(filename, data.table = FALSE)

res_fdata <- qPCR2fdata(data_32HCU)
# Extract column names and create rainbow color to label the data
res_fdata_colnames <- data_32HCU[-1] %>% colnames()
data_colors <- rainbow(length(res_fdata_colnames), alpha = 0.55)
```
In advance the the Cq values were calculated by the following code:

```{r, echo=TRUE, eval=FALSE}
# Load the qpcR package to calculate the Cq values by the second derivative
# maximum method.
options(warn = -1)
library(qpcR)

res_Cq <- sapply(2L:ncol(data_32HCU), function(i) {
  efficiency(pcrfit(data_32HCU, cyc = 1, fluo = i, model = l6))
})

data.frame(
  sample = colnames(data_32HCU)[-1],
  Cq = unlist(res_Cq["cpD2", ]), eff = unlist(res_Cq["eff", ])
)

#        Results
#
# sample    Cq      eff
# 1      A1 14.89 1.092963
# 2      B1 15.68 1.110480
# 3      C1 15.63 1.111474
# ...
# 30     F4 15.71 1.109634
# 31     G4 15.70 1.110373
# 32     H4 15.73 1.117827
```

Next, the amplification curves (\autoref{HCU32}A), the differences between baseline region and plateau region (\autoref{HCU32}B), the correlation between the Cq value and amplification efficiency (\autoref{HCU32}C) and the clusters based on the Hausdorff distance (Figure \ref{HCU32} were taken into account. 32 real-time PCRs for the bacterial gen \textit{aggR} were measured in the VideoScan system. Note: The raw data has not been modified to retain all characteristics of the amplification curves.

Some amplification curves (\autoref{HCU32}A) showed stronger fluctuations and therefore no ideal sigmoid curve progression.  In addition, it can be seen that the amplification curves in the baseline area have a negative non-linear trend and a shift around the zero point. By contrast, the trend in the plateau region appears to be positive. This is similar to the curve shown in \autoref{figure_sigmoid_cuve}. The comparison of the baseline region and the plateau region showed a difference between the 32 amplification curves. The samples `E1`, `F1` and `H1` had the lowest differences between the baseline and plateau regions.  The comparison of Cq values and amplification efficiency showed that most amplification curves exhibit similar behavior. Since there are 32 replicates, the similarity of the Cq values and amplification efficiencies was to be expected. However, there are also amplification curves that show a greater deviation from the median of all Cq values (\autoref{HCU32}C). The analysis by clustering of the Hausdorff distance did not yield any specific pattern (\autoref{HCU32}D).

```{r, echo=TRUE, fig.cap="\\label{HCU32}Clustering of amplification curves. The amplification curves from the 32HCU were processed with the ``qPCR2fdata()`` function and subsequent processed by a cluster analysis and Hausdorff distance analysis. A) Amplification curves were plotted from the raw data. B) Overall, the signal to noise ratios of the amplification curves were comparable between all cavities. C) The Cqs (Second Derivative Maximum) and the amplification efficiency (eff) were calculated with the ``efficiency(pcrfit())`` functions from the ``qpcR`` package. The median Cq is indicated as vertical line. Cqs larger or less than 0.1 of the Cq $\\tilde{x}$ are indicated with the labels of the corresponding sample. D) The clusters according to the Hausdorff distance show no specific pattern regarding the amplification curve signals. It appears that the samples D1, E1, F1, F3, G3 and H1 deviate most from the other amplification curves.", fig.height=8.5, fig.width=11} 

library(fda.usc)
library(magrittr)

calculated_Cqs <- c(
  14.89, 15.68, 15.63, 15.5, 15.54, 15.37, 15.78, 15.24, 15.94,
  15.88, 15.91, 15.77, 15.78, 15.74, 15.84, 15.78, 15.64, 15.61,
  15.66, 15.63, 15.77, 15.71, 15.7, 15.79, 15.8, 15.72, 15.7, 15.82,
  15.62, 15.71, 15.7, 15.73
)

calculated_effs <- c(
  1.09296326515231, 1.11047987547324, 1.11147389307153, 1.10308929700635,
  1.10012176315852, 1.09136717687619, 1.11871308210321, 1.08006168654712,
  1.09500422011318, 1.1078777171126, 1.11269436700649, 1.10628580163733,
  1.1082009954558, 1.11069683827291, 1.11074914659374, 1.10722949813473,
  1.10754282514113, 1.10098387264025, 1.1107026749644, 1.11599641663658,
  1.11388510347017, 1.11398547396991, 1.09410798249025, 1.12422338092929,
  1.11977386646464, 1.11212436173214, 1.12145338871426, 1.12180879952503,
  1.1080276005651, 1.10963449004393, 1.11037302758388, 1.11782689816295
)

# Plot the converted qPCR data
layout(matrix(c(1, 2, 3, 4, 4, 4), 2, 3, byrow = TRUE))
res_fdata %>% plot(
  ., xlab = "Cycle", ylab = "RFU", main = "HCU32_aggR", type = "l",
  lty = 1, lwd = 2, col = data_colors
)
legend(
  "topleft", as.character(res_fdata_colnames), pch = 19,
  col = data_colors, bty = "n", ncol = 4
)
mtext("A", cex = 1.5, side = 3, adj = 0, font = 2)

# Plot the background and plateau phase.

boxplot(
  data_32HCU[, -1] - apply(data_32HCU[, -1], 2, min),
  col = data_colors, las = 2, main = "Signal to noise ratio",
  xlab = "Sample", ylab = "RFU"
)
mtext("B", cex = 1.5, side = 3, adj = 0, font = 2)

# Plot the Cqs and the amplification efficiencies.
# Determine the median of the Cq values and label all Cqs, which a less 0.1 Cqs
# of the median or more then 0.1 Cqs of the median Cq.

plot(
  calculated_Cqs, calculated_effs, xlab = "Cq (SDM)",
  ylab = "eff", main = "Cq vs. Amplification Efficiency",
  type = "p", pch = 19, lty = 1, lwd = 2, col = data_colors
)

median_Cq <- median(calculated_Cqs)
abline(v = median_Cq)

text(median_Cq + 0.01, 1.085, expression(paste(tilde(x))))
labeled <- c(
  which(calculated_Cqs < median_Cq - 0.1),
  which(calculated_Cqs > median_Cq + 0.1)
)

text(
  calculated_Cqs[labeled], calculated_effs[labeled],
  as.character(res_fdata_colnames)[labeled]
)
mtext("C", cex = 1.5, side = 3, adj = 0, font = 2)

# Calculate the Hausdorff distance using the fda.usc package and cluster the
# the distances.

res_fdata_hclust <- metric.hausdorff(res_fdata)
cluster <- hclust(as.dist(res_fdata_hclust))

# plot the distances as clustered data and label the leafs with the Cq values
# and colored dots.

plot(cluster, main = "Clusters of the amplification\n
curves as calculated by the Hausdorff distance")
mtext("D", cex = 1.5, side = 3, adj = 0, font = 2)
```

The analysis gives an overview of the variation of the amplification curve data.

# Summary and Conclusions  \label{section_Summary_and_conclusions}

The \texttt{PCRedux} enables the user to extract features from amplification curve data. Numerous features can be extracted from the amplification curve. Some of them have not been described in the literature. We consider \texttt{PCRedux} as enabling technology for further research. For example, the proposed features are useable for machine learning applications or quality assessment of data.

The ``pcrfit_single()`` function is a wrapper that can be extended if new features 
emerge.

Of note, we would like to emphasis that the functionality of this package is not limited to amplification curve data from qPCR experiments. As stated before, amplification curves have a sigmoid curve shape. Presumably, this can also be used for melting curve analysis.


# References
